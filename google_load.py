import json
import os
import time
import argparse
import re
from datetime import datetime
from typing import List
from tools.google import (
    google_search_filter,
    google_search_page_filter,
    google_search_video_page_filter,
    filter_vet_response,
)
from tools.google_scholar import (
  google_scholar_search_filter,
  google_scholar_search_page_filter
)
from tools.twitter_filter import filter_following_timeline, filter_sidebar_recommendations, filter_explore_content, \
    filter_tweet_detail, filter_suggestions, filter_search_timeline_response, filter_explore_page, \
    filter_generic_timeline, filter_aitrendbrestid_detail, filter_UserTweets, filter_ListLatestTweetsTimeline, \
    filter_ConnectTabTimeline, filter_CommunitiesRankedTimeline, filter_CommunitiesExploreTimeline, \
    filter_CommunitiesFetchOneQuery, filter_CommunityDiscoveryTimeline, filter_TopicTimelineQuery, \
    filter_CommunitiesSearchQuery, filter_community_tweets, filter_ListsManagementPageTimeline,filter_useStoryTopicQuery,filter_TrendRelevantUsers
from tools.wiki_filter import (
    wiki_search_filter, 
    wiki_suggestions_filter, 
    wiki_search_page_filter, 
    extract_wiki_title, 
    inject_content, 
    wiki_content_filter
)
from tools.hugging_face import (
    hugging_face_quick_search_filter,
    hugging_face_card_page_filter,
    hugging_face_models_search_json_filter,
    hugging_face_models_search_page_filter,
    hugging_face_datasets_search_json_filter,
    hugging_face_datasets_search_page_filter,
    hugging_face_spaces_search_json_filter,
    hugging_face_spaces_search_page_filter,
    hugging_face_collections_search_json_filter,
    hugging_face_collections_search_page_filter,
    hugging_face_blogs_search_page_filter,
    hugging_face_blogs_community_page_filter,
    hugging_face_posts_search_page_filter,
    hugging_face_posts_search_json_filter,
    hugging_face_discuss_topics_search_json_filter,
    hugging_face_discuss_topics_search_page_filter,
    hugging_face_discuss_posts_json_filter,
    hugging_face_discuss_posts_page_filter,
    hugging_face_index_page_filter,
    hugging_face_organizations_page_filter,
    hugging_face_fulltext_search_page_filter,
    hugging_face_fulltext_search_json_filter
)
from tools.mongodb import cleanup_old_collections
from tools.web import setup_driver

FILTER_WORDS: List[str] = ["airlines","news","Airlines","airline","ä¹ è¿‘å¹³","å…­å››","é˜²ç«å¢™æŠ€æœ¯","chatgpt","gpt","kimi",'GPT','Kimi']

def response_interceptor(request, response):
    """
    Intercepts responses from Google to filter out unwanted content.
    """
    url = request.url
    content_type = response.headers.get('Content-Type', '')
    try:
        # google scholar
        if 'scholar.google.com' in url: 
            # 1. è¿‡æ»¤æœç´¢å»ºè®®
            if "scholar.google.com/scholar_complete" in url:
                start_time = time.time()
                response.body = google_scholar_search_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤æœç´¢å»ºè®®è€—æ—¶: {duration:.4f}ç§’")
            # 2. è¿‡æ»¤æœç´¢ç»“æœ`
            elif "scholar.google.com/scholar" in url and "text/html" in content_type:
                #è¿‡æ»¤ä¸»é¡µé¢
                start_time = time.time()
                response.body = google_scholar_search_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤ä¸»é¡µé¢è€—æ—¶: {duration:.4f}ç§’")
        # google search
        elif 'google.com' in url:
            if "google.com/search?vet=12" in url:
                start_time = time.time()
                response.body = filter_vet_response(
                    response.body.decode('utf-8', errors='ignore'),
                    FILTER_WORDS
                )
                print(f"è¿‡æ»¤vetè¯·æ±‚è€—æ—¶: {time.time() - start_time:.4f}ç§’")
                return
            # 2. Filter search suggestions
            elif "google.com/complete/search" in url:
                start_time = time.time()
                response.body = google_search_filter(
                    response.body.decode('utf-8', errors='ignore'),
                    FILTER_WORDS
                )
                print(f"è¿‡æ»¤æœç´¢å»ºè®®è€—æ—¶: {time.time() - start_time:.4f}ç§’")
                return
            # 3. Filter HTML search results (main and video pages)
            if "text/html" in content_type:
                start_time = time.time()
                decoded_body = response.body.decode('utf-8', errors='ignore')
                if "udm=7" in url:
                    response.body = google_search_video_page_filter(decoded_body, FILTER_WORDS)
                    print(f"è¿‡æ»¤è§†é¢‘é¡µé¢è€—æ—¶: {time.time() - start_time:.4f}ç§’")
                else:
                    response.body = google_search_page_filter(decoded_body, FILTER_WORDS)
                    print(f"è¿‡æ»¤ä¸»é¡µé¢è€—æ—¶: {time.time() - start_time:.4f}ç§’")
        #wikipedia
        elif "/zh.wikipedia.org" in url:
            #wikiç™¾ç§‘é¦–é¡µæœç´¢æ 
            if "/search/title?q" in url:
                start_time = time.time()
                response.body = wiki_search_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS,
                                                    url)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤wikiç™¾ç§‘æœç´¢è€—æ—¶: {duration:.4f}ç§’")

            #wikiç™¾ç§‘æœç´¢é¡µæœç´¢æ 
            if "w/api.php?action=" in url:
                start_time = time.time()
                response.body = wiki_suggestions_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS,
                                                        url)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤wikiç™¾ç§‘æœç´¢è€—æ—¶: {duration:.4f}ç§’")

            #wikiç™¾ç§‘é¡µé¢
            if "w/index.php?" in url:
                start_time = time.time()
                response.body = wiki_search_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS,
                                                        url)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤wikiç™¾ç§‘æœç´¢è€—æ—¶: {duration:.4f}ç§’")

            #wikiç™¾ç§‘æ ‡é¢˜
            if "/wiki/" in url and 'text/html' in content_type:

                decoded_title = extract_wiki_title(url)

                for word in FILTER_WORDS:
                    if re.search(re.escape(word), decoded_title, re.IGNORECASE):
                        start_time = time.time()
                        response.body = inject_content(response.body.decode('utf-8', errors='ignore'))
                        duration = time.time() - start_time
                        print(f"æ³¨å…¥è€—æ—¶: {duration:.4f}ç§’")
                        break
                if "/Wikipedia:" in url:
                    response.body = wiki_search_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS,
                                                            url)
                else:
                    response.body = wiki_content_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS,
                                                        url)
        #huggingface
        elif 'huggingface.co' in url: 
            # 1. è¿‡æ»¤é¡¶éƒ¨å¿«é€Ÿæœç´¢æ¨èæ¡ç›®
            if "huggingface.co/api/quicksearch" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_quick_search_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤quickæœç´¢å»ºè®®è€—æ—¶: {duration:.4f}ç§’")

            # è¿‡æ»¤å…¨æ–‡æœç´¢ç»“æœé¡µé¢
            elif "huggingface.co/search/full-text" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_fulltext_search_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤full-searché¡µé¢è€—æ—¶: {duration:.4f}ç§’")

            #è¿‡æ»¤å…¨æ–‡æœç´¢ç»“æœjson
            elif "huggingface.co/api/search/full-text" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_fulltext_search_json_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤full-search jsonè€—æ—¶: {duration:.4f}ç§’")

            # 2. è¿‡æ»¤modelsæœç´¢ç»“æœæ¡ç›®ã€modelsé¦–é¡µtrendingå…ƒç´ 
            elif "huggingface.co/models-json" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_models_search_json_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤models-jsonè€—æ—¶: {duration:.4f}ç§’")

            elif "huggingface.co/models" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_models_search_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤models-pageè€—æ—¶: {duration:.4f}ç§’")

            # 3. è¿‡æ»¤datasetsæœç´¢ç»“æœæ¡ç›®
            elif "huggingface.co/datasets-json" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_datasets_search_json_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤datasets-jsonè€—æ—¶: {duration:.4f}ç§’")

            elif "huggingface.co/datasets" in url and "datasets/" not in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_datasets_search_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤datasets-pageè€—æ—¶: {duration:.4f}ç§’")

            # 4. è¿‡æ»¤spacesæœç´¢ç»“æœæ¡ç›®
            elif "huggingface.co/spaces-json" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_spaces_search_json_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤spaces-jsonè€—æ—¶: {duration:.4f}ç§’")

            elif "huggingface.co/spaces" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_spaces_search_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤spaces-pageè€—æ—¶: {duration:.4f}ç§’")

            # 5. è¿‡æ»¤collectionsæœç´¢ç»“æœæ¡ç›®ã€collectionsé¦–é¡µtrendingå…ƒç´ 
            elif "huggingface.co/collections-json" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_collections_search_json_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤collections-jsonè€—æ—¶: {duration:.4f}ç§’")

            elif "huggingface.co/collections" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_collections_search_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤collections-pageè€—æ—¶: {duration:.4f}ç§’")

            # 6. blogsè¿‡æ»¤:å±•å¼€ä¾§è¾¹æ communityé¡µé¢å’Œblogsé¦–é¡µ
            elif "huggingface.co/blog/community" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_blogs_community_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤blog/communityé¡µé¢è€—æ—¶: {duration:.4f}ç§’")


            elif "huggingface.co/blog" in url and "png" not in url and "jpg" not in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_blogs_search_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤blogé¡µé¢è€—æ—¶: {duration:.4f}ç§’")

            # 7. postsè¿‡æ»¤
            elif "huggingface.co/api/posts" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_posts_search_json_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤posts-jsonè€—æ—¶: {duration:.4f}ç§’")

            elif "huggingface.co/posts" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_posts_search_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤postsé¡µé¢è€—æ—¶: {duration:.4f}ç§’")

            # 8. è®ºå›é¡µé¢è¿‡æ»¤ï¼šé¦–é¡µå¸–å­ã€å¸–å­ä¸­çš„posts
            elif "discuss.huggingface.co/latest.json" in url or "discuss.huggingface.co/hot.json" in url or "discuss.huggingface.co/top.json" in url or "discuss.huggingface.co/categories_and_latest" in url or "discuss.huggingface.co/c/" in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_discuss_topics_search_json_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤discuss-topic-jsonè€—æ—¶: {duration:.4f}ç§’")

            elif "discuss.huggingface.co/t/" in url:
                start_time = time.time()
                print(url)
                if "json" in url:
                    response.body = hugging_face_discuss_posts_json_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                else:
                    response.body = hugging_face_discuss_posts_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤discuss-postsé¡µé¢è€—æ—¶: {duration:.4f}ç§’")

            elif "https://discuss.huggingface.co" in url and "text/html" in content_type:
                start_time = time.time()
                print(url)
                response.body = hugging_face_discuss_topics_search_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤discussé¦–é¡µè€—æ—¶: {duration:.4f}ç§’")

            # 9. è¿‡æ»¤models\datasets cardé¡µé¢,å«æœ‰è¿ç¦è¯åˆ™æ— æ³•æ­£å¸¸è®¿é—®é¡µé¢
            elif "https://huggingface.co/" in url and "text/html" in content_type and re.search(re.compile(r'https://huggingface\.co/[^/]+/[^/]+',re.IGNORECASE),url) and "/models" not in url and "/datasets" not in url:
                start_time = time.time()
                print(url)
                response.body = hugging_face_card_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤model\dataset cardé¡µé¢è€—æ—¶: {duration:.4f}ç§’")

            # 10. è¿‡æ»¤organizationsé¡µé¢çš„modelsã€datasetsã€...æ¡ç›®
            elif "https://huggingface.co/" in url and "text/html" in content_type and (re.search(re.compile(r'https://huggingface\.co/[^/]+',re.IGNORECASE),url) or (re.search(re.compile(r'https://huggingface\.co/[^/]+/[^/]+',re.IGNORECASE),url) and ("/models" in url or "/datasets" in url or "/spaces" in url or "/collections" in url))):
                start_time = time.time()
                print(url)
                response.body = hugging_face_organizations_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤organizationé¡µé¢è€—æ—¶: {duration:.4f}ç§’")

            # 11. è¿‡æ»¤huggingface.coé¦–é¡µ
            elif "https://huggingface.co/" in url and "text/html" in content_type:
                start_time = time.time()
                print(url)
                response.body = hugging_face_index_page_filter(response.body.decode('utf-8', errors='ignore'), FILTER_WORDS)
                duration = time.time() - start_time
                print(f"è¿‡æ»¤huggingface.coé¦–é¡µè€—æ—¶: {duration:.4f}ç§’")
        elif 'x.com/i/api/' in request.url:
            print(f"æ£€æµ‹åˆ°APIè¯·æ±‚: {request.url}")
            if 'HomeTimeline' in request.url:
                content_type = response.headers.get('Content-Type', '')
                if 'application/json' in content_type:
                    try:

                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_following_timeline(response.body.decode('utf-8', errors='ignore'),
                                                                  FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif 'SidebarUserRecommendations' in request.url:
                content_type = response.headers.get('Content-Type', '')
                if 'application/json' in content_type:
                    try:

                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_sidebar_recommendations(response.body.decode('utf-8', errors='ignore'),
                                                                     FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif 'ExploreSidebar' in request.url:
                content_type = response.headers.get('Content-Type', '')
                if 'application/json' in content_type:
                    try:
                        # è§£ç å“åº”ä½“
                        response_body = response.body.decode('utf-8', errors='ignore')
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_explore_content(response_body, FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time

                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif 'HomeLatestTimeline?' in request.url:
                content_type = response.headers.get('Content-Type', '')
                if 'application/json' in content_type:
                    try:
                        # è§£ç å“åº”ä½“
                        response_body = response.body.decode('utf-8', errors='ignore')
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_following_timeline(response_body, FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time

                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s ")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")

            elif 'TweetDetail?' in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body, disable_cache = filter_tweet_detail(
                            response.body.decode('utf-8', errors='ignore'),
                            FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time

                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")

            elif "search/typeahead.json" in request.url:
                print("\nğŸ” æ‹¦æˆªåˆ°æœç´¢å»ºè®®è¯·æ±‚:", request.url)

                try:
                    decoded = response.body.decode("utf-8", errors="ignore")
                    data = json.loads(decoded)

                    # åº”ç”¨è¿‡æ»¤
                    filtered = filter_suggestions(data, FILTER_WORDS)
                    response.body = json.dumps(filtered).encode("utf-8")
                    response.headers["content-length"] = str(len(response.body))

                    print(f"  åŸå§‹ç»“æœæ•°: {len(data.get('users', []))}ç”¨æˆ· | {len(data.get('topics', []))} è¯é¢˜")
                    print(f"  è¿‡æ»¤åç»“æœ: {len(filtered.get('users', []))}ç”¨æˆ· | {len(filtered.get('topics', []))}è¯é¢˜")

                except Exception as e:
                    print(f"å¤„ç†æœç´¢å»ºè®®å‡ºé”™: {e}")
            # 2. å¤„ç†æœç´¢ç»“æœï¼ˆGraphQLï¼‰
            elif "SearchTimeline" in request.url:
                print("\n æ‹¦æˆªåˆ°æœç´¢ç»“æœè¯·æ±‚:", request.url)
                content_type = response.headers.get('Content-Type', '')
                if 'application/json' in content_type:
                    try:
                        response_body = response.body.decode('utf-8', errors='ignore')
                        filtered_body = filter_search_timeline_response(response_body, FILTER_WORDS)

                        # æ›´æ–°å“åº”
                        response.body = filtered_body
                        response.headers['content-length'] = str(len(response.body))
                        print(f" å·²è¿‡æ»¤æœç´¢ç»“æœ")

                    except Exception as e:
                        print(f"å¤„ç†æœç´¢å“åº”æ—¶å‡ºé”™: {e}")


            elif "ExplorePage" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(f"\n å¤„ç†æ¢ç´¢é¡µè¯·æ±‚: {request.url}")
                if 'application/json' in content_type:
                    try:

                        # è§£ç å“åº”ä½“
                        response_body = response.body.decode('utf-8', errors='ignore')
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_explore_page(response_body, FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time

                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "GenericTimelineById" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(f"\n å¤„ç†è¶‹åŠ¿é¡µè¯·æ±‚: {request.url}")
                if 'application/json' in content_type:
                    try:
                        # è§£ç å“åº”ä½“
                        response_body = response.body.decode('utf-8', errors='ignore')
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_generic_timeline(response_body, FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time

                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s ")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "AiTrendByRestId?" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:

                        # è§£ç å“åº”ä½“
                        response_body = response.body.decode('utf-8', errors='ignore')
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_aitrendbrestid_detail(response.body.decode('utf-8', errors='ignore'),
                                                                     FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time

                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s ")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "UserTweets?" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:

                        # è§£ç å“åº”ä½“
                        response_body = response.body.decode('utf-8', errors='ignore')
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_UserTweets(response.body.decode('utf-8', errors='ignore'),
                                                          FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time

                        print( f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s ")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "ListLatestTweetsTimeline" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_ListLatestTweetsTimeline(response.body.decode('utf-8', errors='ignore'),
                                                                        FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time

                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s ")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")

            elif "ConnectTabTimeline" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_ConnectTabTimeline(response.body.decode('utf-8', errors='ignore'),
                                                                  FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time

                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s ")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "CommunitiesRankedTimeline" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_CommunitiesRankedTimeline(response.body.decode('utf-8', errors='ignore'),
                                                                         FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time
                        print( f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "CommunitiesExploreTimeline" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_CommunitiesExploreTimeline(
                            response.body.decode('utf-8', errors='ignore'),
                            FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time
                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s ")
                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "CommunitiesFetchOneQuery" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:

                        # è§£ç å“åº”ä½“
                        response_body = response.body.decode('utf-8', errors='ignore')
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_CommunitiesFetchOneQuery(response.body.decode('utf-8', errors='ignore'),
                                                                        FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time
                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s ")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "CommunityDiscoveryTimeline" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_CommunityDiscoveryTimeline(
                            response.body.decode('utf-8', errors='ignore'),
                            FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time
                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s ")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "TopicTimelineQuery" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_TopicTimelineQuery(response.body.decode('utf-8', errors='ignore'),
                                                                  FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time
                        print( f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "CommunitiesSearchQuery" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_CommunitiesSearchQuery(response.body.decode('utf-8', errors='ignore'),
                                                                      FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time
                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s ")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "CommunityTweetsTimeline" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:

                        # è§£ç å“åº”ä½“
                        response_body = response.body.decode('utf-8', errors='ignore')
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_community_tweets(response.body.decode('utf-8', errors='ignore'),
                                                                FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time
                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s ")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "ListsManagementPageTimeline" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_ListsManagementPageTimeline(
                            response.body.decode('utf-8', errors='ignore'),
                            FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time
                        print(f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s ")

                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))

                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "TrendRelevantUsers?" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:
    
                        # è§£ç å“åº”ä½“
                        response_body = response.body.decode('utf-8', errors='ignore')
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_TrendRelevantUsers(response.body.decode('utf-8', errors='ignore'),
                                                                FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time
                        save_api_data(request.url, response.body, "TrendRelevantUsers")
    
                        print(
                            f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s | åŸå§‹å¤§å°: {len(response_body)} | è¿‡æ»¤å: {len(filtered_body)}")
    
                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))
    
                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            elif "useStoryTopicQuery" in request.url:
                content_type = response.headers.get('Content-Type', '')
                print(request.url + " : " + content_type)
                if 'application/json' in content_type:
                    try:
    
                        # è§£ç å“åº”ä½“
                        response_body = response.body.decode('utf-8', errors='ignore')
                        # è¿‡æ»¤æ¨æ–‡
                        start_time = time.time()
                        filtered_body = filter_useStoryTopicQuery(response.body.decode('utf-8', errors='ignore'),
                                                                FILTER_WORDS)
                        response.body = filtered_body.encode('utf-8')
                        duration = time.time() - start_time
                        save_api_data(request.url, response.body, "useStoryTopicQuery")
    
                        print(
                            f"è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s | åŸå§‹å¤§å°: {len(response_body)} | è¿‡æ»¤å: {len(filtered_body)}")
    
                        # æ›´æ–°å“åº”
                        response.headers['content-length'] = str(len(response.body))
    
                    except Exception as e:
                        print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")

    except Exception as e:
        print(f"Error in response_interceptor for URL {url}: {e}")
        pass

def save_responses(driver, output_dir="responses"):
    """
    Saves all requests and their responses to a timestamped file.
    """
    os.makedirs(output_dir, exist_ok=True)
    now = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"response-{now}.txt"
    filepath = os.path.join(output_dir, filename)

    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            for request in getattr(driver, 'requests', []):
                f.write(f'URL: {request.url}\n')
                f.write(f'Method: {getattr(request, "method", "N/A")}\n')
                if hasattr(request, 'response') and request.response:
                    resp = request.response
                    f.write(f'Status code: {getattr(resp, "status_code", "N/A")}\n')
                    f.write(f'Response headers: {getattr(resp, "headers", {})}\n')
                    f.write(f'Response body: {getattr(resp, "body", "")}\n')
                f.write('-' * 60 + '\n')
        print(f"Responses saved to {filepath}")
    except Exception as e:
        print(f"Failed to save responses: {e}")

def main():
    # æ¸…ç†æ—§é›†åˆ
    cleanup_old_collections(days=30)
    parser = argparse.ArgumentParser(description="Load Google with optional proxy.")
    parser.add_argument(
        '--proxy',
        type=str,
        default='127.0.0.1:10809',
        help='Proxy server in format host:port (e.g., 127.0.0.1:10809)'
    )
    args = parser.parse_args()

    try:
        driver = setup_driver(proxy=args.proxy)
        driver.response_interceptor = response_interceptor
        driver.get('https://www.google.com/')
        input("Press Enter to continue...")
        save_responses(driver)
    except Exception as e:
        print(f"Error running driver: {e}")
    finally:
        if 'driver' in locals():
            driver.quit()

if __name__ == "__main__":
    main()
