import json
import re
import time
from datetime import datetime

def contains_forbidden_text(text, forbidden_words):
    if not text or not forbidden_words:
        return False

    lower_text = text.lower()

    for word in forbidden_words:
        pattern = r'(?<![a-z])' + re.escape(word.lower()) + r'(?![a-z])'
        match = re.search(pattern, lower_text)

        if match:
            start, end = match.span()
            context = text[max(0, start - 10):min(len(text), end + 10)]
            print(f"ğŸ”¥ æ£€æµ‹åˆ°è¿ç¦è¯åŒ¹é… | è¯: '{word}' | ä½ç½®: {start}-{end} | ä¸Šä¸‹æ–‡: '...{context}...'")
            return True

    return False


def contains_filter_words(text, forbidden_words):
    """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«ä»»ä½•è¿ç¦è¯ï¼Œè¿”å›åŒ¹é…çš„è¿ç¦è¯åˆ—è¡¨"""
    if not text or not forbidden_words:
        return False

    lower_text = text.lower()

    for word in forbidden_words:
        pattern = r'(?<![a-z])' + re.escape(word.lower()) + r'(?![a-z])'
        match = re.search(pattern, lower_text)

        if match:
            start, end = match.span()
            context = text[max(0, start - 10):min(len(text), end + 10)]
            print(f"ğŸ”¥ æ£€æµ‹åˆ°è¿ç¦è¯åŒ¹é… | è¯: '{word}' | ä½ç½®: {start}-{end} | ä¸Šä¸‹æ–‡: '...{context}...'")
            return word

    return None


def filter_useStoryTopicQuery(response_body, filter_words):
    print("å¼€å§‹è¿‡æ»¤Todayâ€˜s News")
    try:
        # è§£æJSONæ•°æ®
        data = json.loads(response_body)

        # è·å–æ•…äº‹æ¡ç›®åˆ—è¡¨
        items = data.get("data", {}).get("story_topic", {}).get("stories", {}).get("items", [])

        # åˆ›å»ºæ–°åˆ—è¡¨å­˜å‚¨è¿‡æ»¤åçš„æ¡ç›®
        filtered_items = []
        removed_count = 0

        # éå†æ‰€æœ‰æ¡ç›®
        for item in items:
            # è·å–æ ¸å¿ƒå†…å®¹ï¼ˆæ ‡é¢˜+æ‘˜è¦ï¼‰
            result = item.get("trend_results", {}).get("result", {})
            core = result.get("core", {})
            name = core.get("name", "")
            hook = core.get("hook", "")

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•è¿‡æ»¤è¯
            contains_filter_word = (
                    contains_forbidden_text(name, filter_words) or
                    contains_forbidden_text(hook, filter_words)
            )

            # ä¿ç•™æœªåŒ…å«è¿‡æ»¤è¯çš„æ¡ç›®
            if not contains_filter_word:
                filtered_items.append(item)
            else:
                removed_count += 1
                print(f"ğŸ—‘ ç§»é™¤æ¡ç›®: {name}")

        # æ›´æ–°æ•°æ®ç»“æ„
        if "stories" in data["data"]["story_topic"]:
            data["data"]["story_topic"]["stories"]["items"] = filtered_items

        print(f" è¿‡æ»¤å®Œæˆ | åŸå§‹æ¡ç›®: {len(items)} | ä¿ç•™: {len(filtered_items)} | ç§»é™¤: {removed_count}")

        # è¿”å›è¿‡æ»¤åçš„JSON
        return json.dumps(data, separators=(",", ":"))


    except Exception as e:

        print(f"è¿‡æ»¤Today's newså‡ºé”™: {e}")

        return response_body


def filter_suggestions(data, filter_words):
    """è¿‡æ»¤æœç´¢å»ºè®®ä¸­çš„æ•æ„Ÿå†…å®¹"""
    # åˆ›å»ºæ•°æ®å‰¯æœ¬ï¼ˆé¿å…ä¿®æ”¹åŸå§‹æ•°æ®ï¼‰
    filtered_data = data.copy()

    # 1. è¿‡æ»¤ç”¨æˆ·åˆ—è¡¨
    if "users" in filtered_data:
        filtered_users = []
        for user in filtered_data["users"]:
            # æ£€æŸ¥ç”¨æˆ·å­—æ®µæ˜¯å¦åŒ…å«è¿ç¦è¯
            user_text = " ".join([
                user.get("name", ""),
                user.get("screen_name", ""),
                user.get("location", "")
            ]).lower()

            # å¦‚æœæœªæ£€æµ‹åˆ°è¿ç¦è¯åˆ™ä¿ç•™
            if not any(banned_word.lower() in user_text for banned_word in filter_words):
                filtered_users.append(user)

        filtered_data["users"] = filtered_users

    # 2. è¿‡æ»¤è¯é¢˜åˆ—è¡¨
    if "topics" in filtered_data:
        filtered_topics = []
        for topic in filtered_data["topics"]:
            # æ£€æŸ¥è¯é¢˜å­—æ®µæ˜¯å¦åŒ…å«è¿ç¦è¯
            topic_text = topic.get("topic", "").lower()

            # å¦‚æœæœªæ£€æµ‹åˆ°è¿ç¦è¯åˆ™ä¿ç•™
            if not any(banned_word.lower() in topic_text for banned_word in filter_words):
                filtered_topics.append(topic)

        filtered_data["topics"] = filtered_topics

    # 3. æ›´æ–°ç»“æœè®¡æ•°
    filtered_data["num_results"] = (
            len(filtered_data.get("users", [])) +
            len(filtered_data.get("topics", [])) +
            len(filtered_data.get("events", [])) +
            len(filtered_data.get("lists", [])))

    return filtered_data


def filter_search_timeline_response(response_body, filter_words):
    try:
        data = json.loads(response_body)
        instructions = data.get("data", {}).get("search_by_raw_query", {}).get("search_timeline", {}).get("timeline",
                                                                                                          {}).get(
            "instructions", [])

        # åˆå§‹åŒ–ç»Ÿè®¡
        total_entries = 0
        filtered_count = 0
        filtered_details = []

        for instruction in instructions:
            if instruction.get("type") != "TimelineAddEntries":
                continue

            original_count = len(instruction.get("entries", []))
            filtered_entries = []

            for entry in instruction.get("entries", []):
                total_entries += 1
                entry_id = entry.get("entryId", "")
                filtered = False
                filter_reason = ""

                # 1. ç”¨æˆ·æ¨¡å—è¿‡æ»¤
                if "usermodule" in entry_id or entry_id.startswith("user-"):
                    # ç»Ÿä¸€å¤„ç†ç”¨æˆ·æ¨¡å—å’Œç”¨æˆ·æ¡ç›®
                    user_data = None

                    if "usermodule" in entry_id:
                        # ç”¨æˆ·æ¨¡å—ç»“æ„
                        for item in entry.get("content", {}).get("items", []):
                            user_data = item.get("item", {}).get("itemContent", {}).get("user_results", {}).get(
                                "result", {})
                            if user_data:
                                break
                    else:
                        # ç”¨æˆ·æ¡ç›®ç»“æ„
                        content = entry.get("content", {})
                        item_content = content.get("itemContent", {})
                        user_results = item_content.get("user_results", {})
                        user_data = user_results.get("result", {})

                    if user_data:
                        # æ­£ç¡®è·å–ç”¨æˆ·ä¿¡æ¯
                        core = user_data.get("core", {})
                        legacy = user_data.get("legacy", {})
                        screen_name = core.get("screen_name", "")
                        description = legacy.get("description", "")
                        name = core.get("name", "") or legacy.get("name", "")  # å…¼å®¹ä¸¤ç§ä½ç½®

                        # æ£€æŸ¥ç”¨æˆ·ä¿¡æ¯ä¸­çš„è¿ç¦è¯
                        if contains_forbidden_text(screen_name, filter_words):
                            filter_reason = f"ç”¨æˆ·å '{screen_name}' å«è¿ç¦è¯"
                            filtered = True
                        elif contains_forbidden_text(description, filter_words):
                            filter_reason = f"ç”¨æˆ·æè¿°å«è¿ç¦è¯: '{truncate_text(description, 30)}'"
                            filtered = True
                        elif contains_forbidden_text(name, filter_words):
                            filter_reason = f"ç”¨æˆ·æ˜¾ç¤ºå '{name}' å«è¿ç¦è¯"
                            filtered = True

                        if filtered:
                            filtered_details.append(f" ç”¨æˆ·è¿‡æ»¤ | @{screen_name} | åŸå› : {filter_reason}")
                            filtered_count += 1

                # 2. æ¨æ–‡è¿‡æ»¤
                elif entry_id.startswith("tweet-"):
                    tweet = entry.get("content", {}).get("itemContent", {}).get("tweet_results", {}).get("result", {})
                    legacy = tweet.get("legacy", {})
                    core = tweet.get("core", {})

                    # è·å–æ¨æ–‡ä½œè€…ä¿¡æ¯ - ä¿®å¤è·¯å¾„é—®é¢˜
                    user_data = core.get("user_results", {}).get("result", {})
                    if not user_data:
                        # å°è¯•å¤‡ç”¨è·¯å¾„
                        user_data = tweet.get("core", {}).get("user_results", {}).get("result", {})

                    # è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯
                    user_core = user_data.get("core", {})
                    user_legacy = user_data.get("legacy", {})
                    screen_name = user_legacy.get("screen_name", "")
                    description = user_legacy.get("description", "")
                    name = user_core.get("name", "") or user_legacy.get("name", "")

                    # è·å–æ¨æ–‡å†…å®¹
                    tweet_text = legacy.get("full_text", "")

                    # è·å–æ‰€æœ‰æåŠçš„ç”¨æˆ·å
                    user_mentions = []
                    entities = legacy.get("entities", {})
                    if entities:
                        for mention in entities.get("user_mentions", []):
                            user_mentions.append(mention.get("screen_name", ""))

                    # æ£€æŸ¥è¿‡æ»¤æ¡ä»¶
                    # 1. æ£€æŸ¥ç”¨æˆ·ä¿¡æ¯ï¼ˆç”¨æˆ·åã€æ˜¾ç¤ºåã€æè¿°ï¼‰
                    if contains_forbidden_text(screen_name, filter_words):
                        filter_reason = f"ä½œè€…ç”¨æˆ·å @{screen_name} å«è¿ç¦è¯"
                        filtered = True
                    elif contains_forbidden_text(name, filter_words):
                        filter_reason = f"ä½œè€…æ˜¾ç¤ºå '{name}' å«è¿ç¦è¯"
                        filtered = True
                    elif contains_forbidden_text(description, filter_words):
                        filter_reason = f"ä½œè€…æè¿°å«è¿ç¦è¯: '{truncate_text(description, 30)}'"
                        filtered = True

                    # 2. æ£€æŸ¥æ¨æ–‡å†…å®¹
                    elif contains_forbidden_text(tweet_text, filter_words):
                        filter_reason = f"æ¨æ–‡å†…å®¹å«è¿ç¦è¯: '{truncate_text(tweet_text, 40)}'"
                        filtered = True

                    # 3. æ£€æŸ¥æ‰€æœ‰æåŠçš„ç”¨æˆ·å
                    elif any(contains_forbidden_text(mention, filter_words) for mention in user_mentions):
                        # æ‰¾å‡ºå…·ä½“æ˜¯å“ªä¸ªæåŠè§¦å‘äº†è¿‡æ»¤
                        for mention in user_mentions:
                            if contains_forbidden_text(mention, filter_words):
                                filter_reason = f"æåŠç”¨æˆ· @{mention} å«è¿ç¦è¯"
                                filtered = True
                                break

                    # 4. æ£€æŸ¥åª’ä½“æè¿°
                    else:
                        extended_entities = legacy.get("extended_entities", {})
                        media_list = extended_entities.get("media", []) if extended_entities else entities.get("media",
                                                                                                               [])

                        for media in media_list:
                            # æ£€æŸ¥åª’ä½“æè¿°
                            if contains_forbidden_text(media.get("description", ""), filter_words):
                                filter_reason = f"åª’ä½“æè¿°å«è¿ç¦è¯: '{truncate_text(media.get('description', ''), 30)}'"
                                filtered = True
                                break

                            # æ£€æŸ¥é™„åŠ åª’ä½“ä¿¡æ¯
                            additional_info = media.get("additional_media_info", {})
                            if contains_forbidden_text(additional_info.get("title", ""), filter_words) or \
                                    contains_forbidden_text(additional_info.get("description", ""), filter_words):
                                filter_reason = f"åª’ä½“é™„åŠ ä¿¡æ¯å«è¿ç¦è¯"
                                filtered = True
                                break

                    if filtered:
                        tweet_id = tweet.get("rest_id", "")
                        filtered_details.append(f" æ¨æ–‡è¿‡æ»¤ | ID:{tweet_id} | @{screen_name} | åŸå› : {filter_reason}")
                        filtered_count += 1

                # 3. ç¤¾åŒºæ¨¡å—è¿‡æ»¤
                elif "community" in entry_id:
                    for item in entry.get("content", {}).get("items", []):
                        comm_data = item.get("itemContent", {}).get("community_results", {}).get("result", {})
                        comm_name = comm_data.get("name", "")
                        comm_desc = comm_data.get("description", "")

                        if contains_forbidden_text(comm_name, filter_words):
                            filter_reason = f"ç¤¾åŒºåç§° '{comm_name}' å«è¿ç¦è¯"
                            filtered = True
                        elif contains_forbidden_text(comm_desc, filter_words):
                            filter_reason = f"ç¤¾åŒºæè¿°å«è¿ç¦è¯: '{truncate_text(comm_desc, 30)}'"
                            filtered = True

                        if filtered:
                            filtered_details.append(f" ç¤¾åŒºè¿‡æ»¤ | {comm_name} | åŸå› : {filter_reason}")
                            filtered_count += 1
                            break

                # ä¿ç•™æœªè¿‡æ»¤çš„æ¡ç›®
                if not filtered:
                    filtered_entries.append(entry)

            # æ›´æ–°æŒ‡ä»¤ä¸­çš„æ¡ç›®
            instruction["entries"] = filtered_entries
            remaining_count = len(filtered_entries)
            print(
                f" æ¨¡å—å¤„ç† | ç±»å‹: {entry_id.split('-')[0] if '-' in entry_id else entry_id} | åŸå§‹æ¡ç›®: {original_count} | ä¿ç•™: {remaining_count} | è¿‡æ»¤: {original_count - remaining_count}")

        # æœ€ç»ˆç»Ÿè®¡
        print(
            f"\n è¿‡æ»¤å®Œæˆ | æ€»æ¡ç›®: {total_entries} | è¿‡æ»¤: {filtered_count} | ä¿ç•™: {total_entries - filtered_count}")

        if filtered_details:
            print("\n è¿‡æ»¤è¯¦æƒ…:")
            for detail in filtered_details:
                print(f"  - {detail}")
        else:
            print(" æ— å†…å®¹è¢«è¿‡æ»¤")

        return json.dumps(data).encode('utf-8')

    except Exception as e:
        print(f"âŒ è¿‡æ»¤å“åº”æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return response_body.encode('utf-8')



# è¾…åŠ©å‡½æ•°ï¼šæˆªæ–­é•¿æ–‡æœ¬
def truncate_text(text, max_length):
    if len(text) > max_length:
        return text[:max_length] + "..."
    return text


def filter_explore_content(response_body, filter_words):
    """è¿‡æ»¤æ¢ç´¢ä¾§è¾¹æ JSONï¼ˆexplore_content.jsonï¼‰"""
    try:
        data = json.loads(response_body)
        if "data" not in data or "explore_sidebar" not in data["data"]:
            return response_body

        instructions = data["data"]["explore_sidebar"]["timeline"]["instructions"]
        deleted_items = []

        # æŸ¥æ‰¾åŒ…å«entriesçš„instruction
        target_instruction = None
        for instruction in instructions:
            if instruction.get("type") == "TimelineAddEntries":
                target_instruction = instruction
                break

        if not target_instruction or "entries" not in target_instruction:
            return response_body

        entries = target_instruction["entries"]
        new_entries = []

        for entry in entries:
            if entry.get("content", {}).get("__typename") == "TimelineTimelineModule":
                items = entry["content"].get("items", [])
                new_items = []

                for item in items:
                    # ç¡®ä¿itemç»“æ„å­˜åœ¨
                    if not all(key in item for key in ["item", "itemContent"]):
                        new_items.append(item)
                        continue

                    item_content = item["item"]["itemContent"]
                    trend_name = item_content.get("name", "").lower()
                    metadata = item_content.get("trend_metadata", {})
                    domain_context = metadata.get("domain_context", "").lower()

                    if any(word in trend_name or word in domain_context for word in filter_words):
                        deleted_items.append({
                            "id": item.get("entryId", "unknown"),
                            "name": item_content.get("name", "unknown"),
                            "reason": f"Contains filter words in trend ({trend_name}) or domain ({domain_context})"
                        })
                    else:
                        new_items.append(item)

                if new_items:
                    entry["content"]["items"] = new_items
                    new_entries.append(entry)
                else:
                    deleted_items.append({
                        "id": entry.get("entryId", "unknown"),
                        "name": "Entire trend module",
                        "reason": "All items filtered out"
                    })
            else:
                new_entries.append(entry)

        target_instruction["entries"] = new_entries

        # æ‰“å°åˆ é™¤çš„å…ƒç´ 
        if deleted_items:
            print(f"Filtered {len(deleted_items)} items from explore_content:")
            for item in deleted_items:
                print(f"  - {item['id']}: {item['name']} ({item['reason']})")

        return json.dumps(data)

    except Exception as e:
        print(f"Error filtering explore_content: {str(e)}")
        return response_body


def filter_TrendRelevantUsers(response_body, filter_words):

    # è§£æJSONæ•°æ®
    data = json.loads(response_body)

    try:
        # å®šä½åˆ°ç”¨æˆ·æ¡ç›®åˆ—è¡¨
        items = \
        data['data']['ai_trend_by_rest_id']['result']['trend_relevant_users']['timeline']['instructions'][0]['entries'][
            0]['content']['items']

        # åˆ›å»ºæ–°çš„æ¡ç›®åˆ—è¡¨ï¼ˆç”¨äºå­˜å‚¨ä¿ç•™çš„æ¡ç›®ï¼‰
        new_items = []
        removed_count = 0

        # æ£€æŸ¥æ¯ä¸ªç”¨æˆ·æ¡ç›®
        for item in items:
            user_result = item['item']['itemContent']['user_results']['result']
            user_info = {
                'screen_name': user_result['core']['screen_name'],
                'name': user_result['core']['name'],
                'description': user_result.get('legacy', {}).get('description', ''),
                'location': user_result.get('location', {}).get('location', '')
            }

            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡æ»¤è¯
            match_found = False
            for field, value in user_info.items():
                if any(filter_word.lower() in value.lower() for filter_word in filter_words):
                    print(f"åˆ é™¤ç”¨æˆ· @{user_info['screen_name']} - å­—æ®µ [{field}] åŒ…å«è¿‡æ»¤è¯: '{value}'")
                    match_found = True
                    removed_count += 1
                    break

            # ä¿ç•™æœªåŒ¹é…çš„ç”¨æˆ·æ¡ç›®
            if not match_found:
                new_items.append(item)

        # æ›´æ–°æ¡ç›®åˆ—è¡¨
        data['data']['ai_trend_by_rest_id']['result']['trend_relevant_users']['timeline']['instructions'][0]['entries'][
            0]['content']['items'] = new_items

        print(f"å·²è¿‡æ»¤ {removed_count} ä¸ªç”¨æˆ·æ¡ç›®ï¼Œä¿ç•™ {len(new_items)} ä¸ªæ¡ç›®")
        return json.dumps(data, ensure_ascii=False)


    except Exception as e:

        print(f"Error filtering : {str(e)}")

        return response_body


def filter_sidebar_recommendations(response_body, filter_words):
    """è¿‡æ»¤æ¨èå…³æ³¨JSONï¼ˆsidebar_recommendations.jsonï¼‰"""
    try:
        data = json.loads(response_body)
        if "data" not in data or "sidebar_user_recommendations" not in data["data"]:
            return response_body

        recommendations = data["data"]["sidebar_user_recommendations"]
        new_recommendations = []
        deleted_users = []

        for rec in recommendations:
            user = rec["user_results"]["result"]
            screen_name = user["core"]["screen_name"].lower()
            description = user["legacy"]["description"].lower() if "legacy" in user else ""

            if any(word in screen_name or word in description for word in filter_words):
                deleted_users.append({
                    "id": user["rest_id"],
                    "name": user["core"]["name"],
                    "reason": f"Contains filter words in screen_name ({screen_name}) or description ({description})"
                })
            else:
                new_recommendations.append(rec)

        data["data"]["sidebar_user_recommendations"] = new_recommendations

        # æ‰“å°åˆ é™¤çš„ç”¨æˆ·
        if deleted_users:
            print(f"Filtered {len(deleted_users)} users from sidebar_recommendations:")
            for user in deleted_users:
                print(f"  - {user['id']}: {user['name']} ({user['reason']})")

        return json.dumps(data)

    except Exception as e:
        print(f"Error filtering sidebar_recommendations: {str(e)}")
        return response_body


def filter_following_timeline(response_body, filter_words):
    """è¿‡æ»¤é¦–é¡µæ—¶é—´çº¿JSONï¼ˆhome_timeline.jsonï¼‰"""
    try:
        data = json.loads(response_body)
        if "data" not in data or "home" not in data["data"]:
            return response_body

        instructions = data["data"]["home"]["home_timeline_urt"]["instructions"]
        deleted_tweets = []
        filter_words = [word.lower() for word in filter_words]

        for instruction in instructions:
            if instruction["type"] != "TimelineAddEntries":
                continue

            entries = instruction["entries"]
            new_entries = []

            for entry in entries:
                entry_content = entry["content"]
                entry_type = entry_content.get("entryType")
                should_delete = False

                # å¤„ç†å•æ¡æ¨æ–‡
                if entry_type == "TimelineTimelineItem":
                    item_content = entry_content["itemContent"]

                    # è·å–æ¨æ–‡æ•°æ®
                    tweet_data = item_content.get("tweet_results", {}).get("result", {})

                    # æå–æ¨æ–‡æ–‡æœ¬ï¼ˆæ”¯æŒæ™®é€šæ¨æ–‡å’Œé•¿æ–‡æœ¬ï¼‰
                    tweet_text = ""
                    if "legacy" in tweet_data:
                        tweet_text = tweet_data["legacy"].get("full_text", "").lower()
                    elif "note_tweet" in tweet_data:
                        note_tweet = tweet_data["note_tweet"].get("note_tweet_results", {}).get("result", {})
                        tweet_text = note_tweet.get("text", "").lower()

                    # æå–ç”¨æˆ·ä¿¡æ¯
                    user_text = ""
                    user_screen_name = ""
                    if "core" in tweet_data:
                        user_data = tweet_data["core"].get("user_results", {}).get("result", {})
                        if "legacy" in user_data:
                            legacy = user_data["legacy"]
                            user_screen_name = legacy.get("screen_name", "").lower()
                            user_name = legacy.get("name", "").lower()
                            description = legacy.get("description", "").lower()
                            user_text = f"{user_screen_name} {user_name} {description}"

                    # æ£€æŸ¥è½¬æ¨çš„åŸå§‹å†…å®¹
                    if "legacy" in tweet_data:
                        legacy = tweet_data["legacy"]
                        if "retweeted_status_result" in legacy:
                            retweet_data = legacy["retweeted_status_result"].get("result", {})
                            if "legacy" in retweet_data:
                                retweet_text = retweet_data["legacy"].get("full_text", "").lower()
                                tweet_text += " " + retweet_text

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡æ»¤è¯
                    full_text = f"{tweet_text} {user_text}".lower()
                    if any(word in full_text for word in filter_words):
                        deleted_tweets.append({
                            "id": entry["entryId"],
                            "text": tweet_text[:50] + "..." if tweet_text else "No text",
                            "user": user_screen_name if user_screen_name else "Unknown"
                        })
                        should_delete = True

                # å¤„ç†å¯¹è¯çº¿ç¨‹
                elif entry_type == "TimelineTimelineModule":
                    items = entry_content["items"]
                    new_items = []

                    for item in items:
                        item_content = item["item"]["itemContent"]
                        item_tweet_data = item_content.get("tweet_results", {}).get("result", {})

                        # æå–æ¨æ–‡æ–‡æœ¬
                        item_tweet_text = ""
                        if "legacy" in item_tweet_data:
                            item_tweet_text = item_tweet_data["legacy"].get("full_text", "").lower()
                        elif "note_tweet" in item_tweet_data:
                            note_tweet = item_tweet_data["note_tweet"].get("note_tweet_results", {}).get("result", {})
                            item_tweet_text = note_tweet.get("text", "").lower()

                        # æå–ç”¨æˆ·ä¿¡æ¯
                        item_user_text = ""
                        item_user_screen_name = ""
                        if "core" in item_tweet_data:
                            item_user_data = item_tweet_data["core"].get("user_results", {}).get("result", {})
                            if "legacy" in item_user_data:
                                legacy = item_user_data["legacy"]
                                item_user_screen_name = legacy.get("screen_name", "").lower()
                                user_name = legacy.get("name", "").lower()
                                description = legacy.get("description", "").lower()
                                item_user_text = f"{item_user_screen_name} {user_name} {description}"

                        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡æ»¤è¯
                        item_full_text = f"{item_tweet_text} {item_user_text}".lower()
                        if any(word in item_full_text for word in filter_words):
                            deleted_tweets.append({
                                "id": item["entryId"],
                                "text": item_tweet_text[:50] + "..." if item_tweet_text else "No text",
                                "user": item_user_screen_name if item_user_screen_name else "Unknown"
                            })
                        else:
                            new_items.append(item)

                    # æ›´æ–°å¯¹è¯çº¿ç¨‹ä¸­çš„æ¡ç›®
                    entry_content["items"] = new_items
                    should_delete = len(new_items) == 0

                # ä¿ç•™ä¸éœ€è¦åˆ é™¤çš„æ¡ç›®
                if not should_delete:
                    new_entries.append(entry)

            # æ›´æ–°æŒ‡ä»¤ä¸­çš„æ¡ç›®åˆ—è¡¨
            instruction["entries"] = new_entries

        # æ‰“å°åˆ é™¤çš„æ¨æ–‡
        if deleted_tweets:
            print(f"Filtered {len(deleted_tweets)} tweets from home_timeline:")
            for tweet in deleted_tweets:
                print(f"  - {tweet['id']} by @{tweet['user']}: {tweet['text']}")

        return json.dumps(data, ensure_ascii=False)

    except Exception as e:
        print(f"Error filtering home_timeline: {str(e)}")
        return response_body


def filter_explore_page(json_str, filter_words):
    try:
        data = json.loads(json_str)
        removed_count = 0
        removed_details = []

        # å¢å¼ºç‰ˆæ—¥å¿—è®°å½•å‡½æ•°
        def log_removal(entry_id, entry_type, filter_reason, content_sample):
            nonlocal removed_count
            removed_count += 1
            # æ„å»ºè¯¦ç»†çš„æ—¥å¿—å¯¹è±¡
            detail = {
                "id": entry_id,
                "type": entry_type,
                "reason": filter_reason,
                "content": content_sample
            }
            removed_details.append(detail)

        # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«è¿‡æ»¤è¯
        def contains_filter_words(text):
            if not text: return None
            for word in filter_words:
                if re.search(rf'\b{re.escape(word)}\b', text, re.IGNORECASE):
                    return word
            return None

        # ä¸»è¿‡æ»¤å‡½æ•°
        def filter_entries(entries):
            filtered = []
            for entry in entries:
                content = entry.get("content")
                entry_id = entry.get("entryId", "unknown")

                # è·³è¿‡æ— å†…å®¹æ¡ç›®
                if not content:
                    filtered.append(entry)
                    continue

                # 1. å¤„ç†æ¨æ–‡ç±»å‹
                if (content.get("entryType") == "TimelineTimelineItem" and
                        content.get("itemContent", {}).get("itemType") == "TimelineTweet"):

                    tweet = content["itemContent"].get("tweet_results", {}).get("result", {})
                    text = tweet.get("legacy", {}).get("full_text", "")

                    if filter_reason := contains_filter_words(text):
                        # è®°å½•è¯¦ç»†æ—¥å¿—
                        preview = text[:100] + "..." if len(text) > 100 else text
                        log_removal(entry_id, "æ¨æ–‡", f"åŒ¹é…å…³é”®è¯: {filter_reason}", preview)
                        continue

                # 2. å¤„ç†è¶‹åŠ¿/æ–°é—»æ¡ç›®
                elif (content.get("entryType") == "TimelineTimelineItem" and
                      content.get("itemContent", {}).get("itemType") == "TimelineTrend"):

                    trend = content["itemContent"]
                    text = trend.get("name", "")
                    meta_desc = trend.get("trend_metadata", {}).get("meta_description", "")
                    full_text = f"{text} {meta_desc}"

                    if filter_reason := contains_filter_words(full_text):
                        # è®°å½•è¯¦ç»†æ—¥å¿—
                        preview = f"{text} ({meta_desc})"
                        log_removal(entry_id, "è¶‹åŠ¿è¯é¢˜", f"åŒ¹é…å…³é”®è¯: {filter_reason}", preview)
                        continue

                # 3. å¤„ç†ç”¨æˆ·æ¨èæ¨¡å—
                elif (content.get("entryType") == "TimelineTimelineModule" and
                      entry_id.startswith("who-to-follow-")):

                    # è¿‡æ»¤æ¨¡å—å†…çš„æ¯ä¸ªç”¨æˆ·
                    filtered_items = []
                    for item in content.get("items", []):
                        item_id = item.get("entryId", "unknown")
                        user = item.get("item", {}).get("itemContent", {}).get("user_results", {}).get("result", {})
                        name = user.get("legacy", {}).get("name", "")
                        desc = user.get("legacy", {}).get("description", "")
                        text = f"{name} {desc}"

                        if filter_reason := contains_filter_words(text):
                            # è®°å½•è¯¦ç»†æ—¥å¿—
                            preview = f"{name}: {desc[:50]}..." if desc else name
                            log_removal(item_id, "æ¨èç”¨æˆ·", f"åŒ¹é…å…³é”®è¯: {filter_reason}", preview)
                            continue

                        filtered_items.append(item)

                    # å¤„ç†ç©ºæ¨¡å—
                    if not filtered_items:
                        header = content.get("header", {}).get("text", "ç”¨æˆ·æ¨è")
                        log_removal(entry_id, "æ¨èæ¨¡å—", "æ‰€æœ‰ç”¨æˆ·è¢«è¿‡æ»¤", f"æ¨¡å—æ ‡é¢˜: {header}")
                        continue

                    # æ›´æ–°æ¨¡å—å†…å®¹
                    content["items"] = filtered_items
                    entry["content"] = content

                # 4. å¤„ç†æ–°é—»æ•…äº‹æ¨¡å—
                elif (content.get("entryType") == "TimelineTimelineModule" and
                      entry_id.startswith("stories-")):

                    # è¿‡æ»¤æ¨¡å—å†…çš„æ–°é—»
                    filtered_items = []
                    for item in content.get("items", []):
                        item_id = item.get("entryId", "unknown")
                        trend = item.get("item", {}).get("itemContent", {})
                        name = trend.get("name", "")
                        context = trend.get("social_context", {}).get("text", "") if isinstance(
                            trend.get("social_context"), dict) else ""
                        text = f"{name} {context}"

                        if filter_reason := contains_filter_words(text):
                            # è®°å½•è¯¦ç»†æ—¥å¿—
                            preview = f"{name} | {context}"
                            log_removal(item_id, "æ–°é—»æ•…äº‹", f"åŒ¹é…å…³é”®è¯: {filter_reason}", preview)
                            continue

                        filtered_items.append(item)

                    # å¤„ç†ç©ºæ¨¡å—
                    if not filtered_items:
                        header = content.get("header", {}).get("text", "ä»Šæ—¥æ–°é—»")
                        log_removal(entry_id, "æ–°é—»æ¨¡å—", "æ‰€æœ‰æ•…äº‹è¢«è¿‡æ»¤", f"æ¨¡å—æ ‡é¢˜: {header}")
                        continue

                    # æ›´æ–°æ¨¡å—å†…å®¹
                    content["items"] = filtered_items
                    entry["content"] = content

                # ä¿ç•™å…¶ä»–ç±»å‹æ¡ç›®
                filtered.append(entry)

            return filtered

        # 1. å¤„ç†åˆå§‹æ—¶é—´çº¿
        explore_body = data.get("data", {}).get("explore_page", {}).get("body", {})
        if initial_timeline := explore_body.get("initialTimeline"):
            if timeline_obj := initial_timeline.get("timeline", {}).get("timeline"):
                for instruction in timeline_obj.get("instructions", []):
                    if instruction.get("type") == "TimelineAddEntries":
                        instruction["entries"] = filter_entries(instruction.get("entries", []))

        # 2. å¤„ç†æ‰€æœ‰åˆ†ç±»æ—¶é—´çº¿
        for timeline in explore_body.get("timelines", []):
            if timeline_data := timeline.get("timeline"):
                for instruction in timeline_data.get("instructions", []):
                    if instruction.get("type") == "TimelineAddEntries":
                        instruction["entries"] = filter_entries(instruction.get("entries", []))

        # æ‰“å°åˆ é™¤çš„æ¡ç›®ä¿¡æ¯
        if removed_details:
            print(f" å·²åˆ é™¤ {removed_count} ä¸ªè¿è§„æ¡ç›®:")
            for detail in removed_details:
                print(f"   - ID: {detail['id']}")
                print(f"     ç±»å‹: {detail['type']}, åŸå› : {detail['reason']}")
                print(f"     å†…å®¹: {detail['content']}")

        return json.dumps(data, ensure_ascii=False)

    except Exception as e:
        print(f"JSONå¤„ç†é”™è¯¯: {e}")
        return json_str


def filter_generic_timeline(response_body, filter_words):
    """
    è¿‡æ»¤Twitterè¶‹åŠ¿æ•°æ®ï¼Œç§»é™¤åŒ…å«å…³é”®è¯çš„è¶‹åŠ¿æ¡ç›®
    """
    try:
        # è§£æJSONæ•°æ®
        data = json.loads(response_body)

        # è·å–è¶‹åŠ¿æ¡ç›®åˆ—è¡¨è·¯å¾„
        instructions = data["data"]["timeline"]["timeline"]["instructions"]
        print(f" æ‰¾åˆ° {len(instructions)} æ¡æŒ‡ä»¤")

        # å¯»æ‰¾åŒ…å«è¶‹åŠ¿çš„TimelineAddEntriesæŒ‡ä»¤
        trend_count = 0
        filtered_count = 0
        preserved_count = 0

        for instruction in instructions:
            if instruction["type"] == "TimelineAddEntries":
                entries = instruction["entries"]
                new_entries = []
                print(f" å¼€å§‹å¤„ç† {len(entries)} ä¸ªæ¡ç›®...")

                # è¿‡æ»¤æ¯ä¸ªè¶‹åŠ¿æ¡ç›®
                for entry in entries:
                    # å¤„ç†éè¶‹åŠ¿æ¡ç›®ï¼ˆå¦‚æ¸¸æ ‡ï¼‰
                    if not entry["entryId"].startswith("trend-"):
                        new_entries.append(entry)
                        continue

                    trend_count += 1
                    # è·å–è¶‹åŠ¿å†…å®¹
                    content = entry["content"]
                    item_content = content.get("itemContent", {})

                    # æ£€æŸ¥æ˜¯å¦ä¸ºè¶‹åŠ¿æ¡ç›®
                    if item_content.get("itemType") == "TimelineTrend":
                        name = item_content.get("name", "")
                        metadata = item_content.get("trend_metadata", {})
                        description = metadata.get("meta_description", "")

                        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡æ»¤å…³é”®è¯
                        should_filter = any(
                            keyword.lower() in name.lower() or
                            keyword.lower() in description.lower()
                            for keyword in filter_words
                        )

                        # è¾“å‡ºè¶‹åŠ¿ä¿¡æ¯
                        rank = item_content.get("rank", "?")
                        domain = metadata.get("domain_context", "")
                        trend_info = f"#{rank} {name} | {domain} | {description}"

                        # ä¿ç•™ä¸åŒ…å«å…³é”®è¯çš„æ¡ç›®
                        if not should_filter:
                            preserved_count += 1
                            new_entries.append(entry)
                            print(f" ä¿ç•™è¶‹åŠ¿: {trend_info}")
                        else:
                            filtered_count += 1
                            print(f" è¿‡æ»¤è¶‹åŠ¿: {trend_info}")
                    else:
                        new_entries.append(entry)

                # æ›´æ–°æ¡ç›®åˆ—è¡¨
                instruction["entries"] = new_entries
                print(f" æ¡ç›®æ›´æ–°å®Œæˆ: åŸå§‹ {len(entries)} â†’ å½“å‰ {len(new_entries)}")

        # æ‰“å°è¿‡æ»¤æ‘˜è¦
        print(f"\n è¿‡æ»¤æ‘˜è¦:")
        print(f"â€¢ æ€»è¶‹åŠ¿æ¡ç›®: {trend_count}")
        print(f"â€¢ è¿‡æ»¤æ¡ç›®æ•°: {filtered_count} (å…³é”®è¯: {', '.join(filter_words)})")
        print(f"â€¢ ä¿ç•™æ¡ç›®æ•°: {preserved_count}")

        # è¿”å›è¿‡æ»¤åçš„JSON
        return json.dumps(data, ensure_ascii=False)

    except Exception as e:
        print(f" è¿‡æ»¤è¶‹åŠ¿æ•°æ®æ—¶å‡ºé”™: {e}")
        # å‡ºé”™æ—¶è¿”å›åŸå§‹æ•°æ®
        return response_body


def filter_aitrendbrestid_detail(json_str, filter_words):
    """
    è¿‡æ»¤åŒ…å«å…³é”®è¯çš„AIè¶‹åŠ¿é¡µé¢å†…å®¹
    """
    print(f"å¼€å§‹è¿‡æ»¤AiTrendByRestIdå†…å®¹ï¼Œå…³é”®è¯: {filter_words}")

    try:
        data = json.loads(json_str)
        print("JSONè§£ææˆåŠŸï¼Œå¼€å§‹æ£€æŸ¥æ–‡ç« å†…å®¹...")

        # å®šä½åˆ°æ–‡ç« å†…å®¹
        article_text = data.get("data", {}).get("ai_trend_by_rest_id", {}).get("result", {}).get("page", {}).get(
            "article", {}).get("article_text", {}).get("text", "")

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡æ»¤è¯
        found_keywords = []
        for word in filter_words:
            if word and word.lower() in article_text.lower():
                found_keywords.append(word)

        if found_keywords:
            print(f" æ£€æµ‹åˆ°è¿‡æ»¤å…³é”®è¯: {found_keywords}ï¼Œå¼€å§‹æ¸…ç©ºé¡µé¢å†…å®¹...")

            # æ¸…ç©ºæ•´ä¸ªé¡µé¢å†…å®¹
            page = data["data"]["ai_trend_by_rest_id"]["result"].get("page", {})
            if page:
                # è®°å½•åŸå§‹å†…å®¹é•¿åº¦ç”¨äºå¯¹æ¯”
                orig_title_len = len(page["article"]["title"]) if "article" in page else 0
                orig_text_len = len(page["article"]["article_text"]["text"]) if "article" in page else 0

                # æ¸…ç©ºæ–‡ç« å†…å®¹
                if "article" in page:
                    page["article"]["title"] = ""
                    page["article"]["article_text"]["text"] = ""
                    page["article"]["article_text"]["entities"] = []

                # æ¸…ç©ºæ—¶é—´çº¿
                orig_timelines = len(page.get("post_timelines", []))
                page["post_timelines"] = []

                # æ¸…ç©ºå…¶ä»–å­—æ®µ
                page["disclaimer"] = ""
                page["available_actions"] = []

                disable_cache = True
                print(
                    f" é¡µé¢å†…å®¹å·²æ¸…ç©º | æ ‡é¢˜: {orig_title_len}â†’0 å­—ç¬¦ | æ­£æ–‡: {orig_text_len}â†’0 å­—ç¬¦ | æ—¶é—´çº¿: {orig_timelines}â†’0 æ¡")
        else:
            print("æœªæ£€æµ‹åˆ°è¿‡æ»¤å…³é”®è¯ï¼Œä¿ç•™åŸå§‹å†…å®¹")

        return json.dumps(data, ensure_ascii=False)

    except Exception as e:
        print(f" JSONè§£æé”™è¯¯: {e}ï¼Œè¿”å›åŸå§‹å†…å®¹")
        return json_str


def filter_UserTweets(json_str, filter_words):
    start_time = time.time()
    original_size = len(json_str)
    filtered_count = 0
    recommended_filtered = 0
    total_tweets = 0
    total_recommended = 0

    try:
        # è§£æJSONä¸ºPythonå¯¹è±¡
        data = json.loads(json_str)

        # è·å–instructionsåˆ—è¡¨
        instructions = data["data"]["user"]["result"]["timeline"]["timeline"]["instructions"]

        # éå†æ‰€æœ‰æŒ‡ä»¤
        for instruction in instructions:
            if instruction.get("type") == "TimelineAddEntries":
                entries = instruction["entries"]
                filtered_entries = []

                for entry in entries:
                    entry_id = entry["entryId"]

                    # å¤„ç†æ¨æ–‡æ¡ç›®
                    if entry_id.startswith("tweet-"):
                        total_tweets += 1
                        try:
                            # è·å–æ¨æ–‡æ–‡æœ¬
                            tweet_content = entry["content"]["itemContent"]["tweet_results"]["result"]
                            legacy = tweet_content.get("legacy", {})
                            full_text = legacy.get("full_text", "").lower()

                            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡æ»¤è¯
                            contains_filter_word = any(
                                word.lower() in full_text
                                for word in filter_words
                            )

                            if contains_filter_word:
                                filtered_count += 1
                                continue  # è·³è¿‡åŒ…å«è¿‡æ»¤è¯çš„æ¨æ–‡

                            filtered_entries.append(entry)

                        except KeyError:
                            # ç»“æ„å¼‚å¸¸æ—¶ä¿ç•™æ¡ç›®
                            filtered_entries.append(entry)

                    # å¤„ç†æ¨èå…³æ³¨æ¨¡å—
                    elif entry_id.startswith("who-to-follow-"):
                        total_recommended += 1
                        try:
                            items = entry["content"]["items"]
                            filtered_items = []

                            for item in items:
                                try:
                                    user_result = item["itemContent"]["user_results"]["result"]
                                    legacy = user_result.get("legacy", {})
                                    description = legacy.get("description", "").lower()

                                    # æ£€æŸ¥ç®€ä»‹æ˜¯å¦åŒ…å«è¿‡æ»¤è¯
                                    contains_filter_word = any(
                                        word.lower() in description
                                        for word in filter_words
                                    )

                                    if contains_filter_word:
                                        recommended_filtered += 1
                                        continue  # è·³è¿‡åŒ…å«è¿‡æ»¤è¯çš„ç”¨æˆ·

                                    filtered_items.append(item)

                                except KeyError:
                                    # ç»“æ„å¼‚å¸¸æ—¶ä¿ç•™æ¡ç›®
                                    filtered_items.append(item)

                            # æ›´æ–°è¿‡æ»¤åçš„é¡¹ç›®
                            if filtered_items:
                                entry["content"]["items"] = filtered_items
                                filtered_entries.append(entry)
                            else:
                                # å¦‚æœæ‰€æœ‰æ¨èç”¨æˆ·éƒ½è¢«è¿‡æ»¤ï¼Œåˆ™ç§»é™¤æ•´ä¸ªæ¨¡å—
                                recommended_filtered += 1

                        except KeyError:
                            # ç»“æ„å¼‚å¸¸æ—¶ä¿ç•™æ¡ç›®
                            filtered_entries.append(entry)

                    # ä¿ç•™å…¶ä»–ç±»å‹æ¡ç›®
                    else:
                        filtered_entries.append(entry)

                # æ›´æ–°è¿‡æ»¤åçš„æ¡ç›®
                instruction["entries"] = filtered_entries

        # åºåˆ—åŒ–å›JSON
        filtered_json = json.dumps(data, ensure_ascii=False)
        filtered_size = len(filtered_json)

        # æ‰“å°è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯
        duration = time.time() - start_time
        print(f"[UserTweetsè¿‡æ»¤] è€—æ—¶: {duration:.4f}s | "
              f"åŸå§‹: {original_size}å­—èŠ‚ | è¿‡æ»¤å: {filtered_size}å­—èŠ‚ | "
              f"è¿‡æ»¤æ¨æ–‡: {filtered_count}/{total_tweets} | "
              f"è¿‡æ»¤æ¨èç”¨æˆ·: {recommended_filtered}/{total_recommended}")

        return filtered_json

    except (json.JSONDecodeError, KeyError) as e:
        print(f"[UserTweetsè¿‡æ»¤] é”™è¯¯: {e} | è¿”å›åŸå§‹æ•°æ®")
        return json_str  # è§£æå¤±è´¥æ—¶è¿”å›åŸå§‹æ•°æ®


def filter_ConnectTabTimeline(json_str, filter_words):
    start_time = time.time()
    original_length = len(json_str)
    print(f"å¼€å§‹è¿‡æ»¤ConnectTabTimelineæ•°æ® | åŸå§‹é•¿åº¦: {original_length} å­—ç¬¦ | è¿‡æ»¤è¯: {filter_words}")

    if not json_str or not filter_words:
        print("è­¦å‘Š: è¾“å…¥ä¸ºç©ºæˆ–æ— è¿‡æ»¤è¯é…ç½®")
        return json_str

    try:
        data = json.loads(json_str)
        total_users = 0
        removed_users = 0

        # è·å–æŒ‡ä»¤é›†
        instructions = data.get("data", {}).get("connect_tab_timeline", {}).get("timeline", {}).get("instructions", [])

        for instruction in instructions:
            if instruction.get("type") == "TimelineAddEntries":
                entries = instruction.get("entries", [])

                for entry in entries:
                    entry_id = entry.get("entryId", "")

                    # æ‰©å±•æ”¯æŒçš„æ¡ç›®ç±»å‹
                    if any(entry_id.startswith(prefix) for prefix in [
                        "creators-only-connect-tab-",
                        "mergeallcandidatesmodule-"
                    ]):
                        items = []

                        # å¤„ç†ä¸åŒç»“æ„çš„æ•°æ®å®¹å™¨
                        content = entry.get("content", {})
                        if "items" in content:  # ç›´æ¥åŒ…å«items
                            items = content["items"]
                        elif "items" in content.get("content", {}):  # åµŒå¥—åœ¨contentå†…
                            items = content["content"]["items"]

                        total_users += len(items)

                        # å€’åºè¿‡æ»¤
                        new_items = []
                        for item in items:
                            user_data = item.get("item", {}).get("itemContent", {}).get("user_results", {}).get(
                                "result", {})

                            if not user_data:
                                # å°è¯•æ›¿ä»£è·¯å¾„
                                user_data = item.get("content", {}).get("itemContent", {}).get("user_results", {}).get(
                                    "result", {})

                            if not user_data:
                                new_items.append(item)  # ä¿ç•™æ— æ³•è§£æçš„æ¡ç›®
                                continue

                            # è·å–ç”¨æˆ·ä¿¡æ¯
                            core = user_data.get("core", {})
                            legacy = user_data.get("legacy", {})
                            screen_name = core.get("screen_name", "æœªçŸ¥ç”¨æˆ·")
                            user_id = user_data.get("rest_id", "æœªçŸ¥ID")

                            # æ£€æŸ¥å­—æ®µ
                            text_fields = [
                                core.get("name", ""),
                                screen_name,
                                legacy.get("description", ""),
                                user_data.get("location", {}).get("location", "")
                            ]

                            # æ£€æŸ¥è¿‡æ»¤è¯
                            should_remove = False
                            matched_word = ""
                            matched_field = ""

                            for field in text_fields:
                                if not field:
                                    continue

                                for word in filter_words:
                                    if re.search(rf'\b{re.escape(word)}\b', field, flags=re.IGNORECASE):
                                        should_remove = True
                                        matched_word = word
                                        matched_field = field[:50] + "..." if len(field) > 50 else field
                                        break

                                if should_remove:
                                    break

                            if should_remove:
                                removed_users += 1
                                print(f"âš ï¸ ç§»é™¤ç”¨æˆ·: @{screen_name} (ID:{user_id})")
                                print(f"   åŒ¹é…è¯: '{matched_word}' | å­—æ®µå†…å®¹: '{matched_field}'")
                            else:
                                new_items.append(item)

                        # æ›´æ–°æ¡ç›®å†…å®¹
                        if "content" in content and "items" in content["content"]:
                            content["content"]["items"] = new_items
                        else:
                            content["items"] = new_items

        # è½¬æ¢å›JSON
        result_json = json.dumps(data, ensure_ascii=False)
        new_length = len(result_json)

        # æ‰“å°ç»Ÿè®¡
        process_time = time.time() - start_time
        preserved_users = total_users - removed_users
        print(f"è¿‡æ»¤å®Œæˆ | è€—æ—¶: {process_time:.3f}s")
        print(f"ç”¨æˆ·ç»Ÿè®¡: æ€»æ•°={total_users} | ä¿ç•™={preserved_users} | ç§»é™¤={removed_users}")
        print(f"æ•°æ®é•¿åº¦: åŸå§‹={original_length} | å¤„ç†å={new_length} | ç¼©å‡={original_length - new_length}å­—ç¬¦")

        return result_json
    except Exception as e:
        print(f" å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        print("è¿”å›åŸå§‹æ•°æ®")
        return json_str


def filter_ListLatestTweetsTimeline(json_str, filter_words):
    """
    è¿‡æ»¤Twitteræ—¶é—´çº¿JSONæ•°æ®ï¼Œåˆ é™¤åŒ…å«æŒ‡å®šå…³é”®è¯çš„æ¡ç›®ï¼ˆåŒ…æ‹¬éæ¨æ–‡æ¡ç›®ï¼‰
    """
    if not json_str or not filter_words:
        print("æ— éœ€è¿‡æ»¤: ç©ºæ•°æ®æˆ–ç©ºå…³é”®è¯åˆ—è¡¨")
        return json_str

    try:
        # è§£æJSONæ•°æ®
        data = json.loads(json_str)
        original_count = 0
        filtered_count = 0

        # ç¼–è¯‘å…³é”®è¯æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¿½ç•¥å¤§å°å†™
        pattern = re.compile('|'.join(map(re.escape, filter_words)), re.IGNORECASE)
        print(f"å¼€å§‹è¿‡æ»¤ï¼Œå…³é”®è¯: {', '.join(filter_words)}")

        # éå†æ‰€æœ‰æŒ‡ä»¤
        for instruction in data.get('data', {}).get('list', {}).get('tweets_timeline', {}).get('timeline', {}).get(
                'instructions', []):
            if instruction.get('type') == 'TimelineAddEntries':
                # åˆ›å»ºæ–°çš„æ¡ç›®åˆ—è¡¨ï¼ˆåªä¿ç•™ä¸å«å…³é”®è¯çš„æ¡ç›®ï¼‰
                new_entries = []
                entries = instruction.get('entries', [])
                original_count = len(entries)

                for entry in entries:
                    entry_id = entry.get('entryId', '')
                    should_keep = True

                    # æå–æ‰€æœ‰å¯èƒ½çš„æ–‡æœ¬å†…å®¹
                    texts = extract_text_from_entry(entry)

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                    for text in texts:
                        if text and pattern.search(text):
                            print(f"è¿‡æ»¤æ¡ç›® {entry_id}: åŒ…å«å…³é”®è¯ - {text[:50]}...")
                            should_keep = False
                            filtered_count += 1
                            break

                    if should_keep:
                        new_entries.append(entry)

                # æ›´æ–°æ¡ç›®åˆ—è¡¨
                instruction['entries'] = new_entries

        # æ‰“å°è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯
        print(f"è¿‡æ»¤å®Œæˆ: åŸå§‹æ¡ç›®æ•°={original_count}, ä¿ç•™æ¡ç›®æ•°={len(new_entries)}, è¿‡æ»¤æ¡ç›®æ•°={filtered_count}")

        # è¿”å›è¿‡æ»¤åçš„JSON
        return json.dumps(data, ensure_ascii=False)

    except json.JSONDecodeError as e:
        print(f"JSONè§£æå¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹æ•°æ®")
        return json_str
    except Exception as e:
        print(f"è¿‡æ»¤è¿‡ç¨‹ä¸­å‡ºé”™: {e}ï¼Œè¿”å›åŸå§‹æ•°æ®")
        return json_str


def extract_text_from_entry(entry):
    """
    ä»æ¡ç›®ä¸­æå–æ‰€æœ‰å¯èƒ½çš„æ–‡æœ¬å†…å®¹
    """
    texts = []

    try:
        # é€šç”¨æ–‡æœ¬æå–è·¯å¾„ï¼ˆé€‚ç”¨äºæ‰€æœ‰æ¡ç›®ç±»å‹ï¼‰
        content = entry.get('content', {})

        # 1. ç›´æ¥æ–‡æœ¬å­—æ®µ
        for field in ['value', 'text', 'title', 'description', 'header', 'footer', 'displayText']:
            if field in content:
                texts.append(str(content[field]))

        # 2. åµŒå¥—æ–‡æœ¬å­—æ®µ
        for field in ['primaryText', 'secondaryText']:
            if field in content.get('header', {}):
                texts.append(str(content['header'][field].get('text', '')))

        # 3. é¡¹ç›®åˆ—è¡¨ä¸­çš„æ–‡æœ¬
        for item in content.get('items', []):
            item_content = item.get('item', {}).get('itemContent', {})
            for field in ['displayText', 'text', 'title', 'description']:
                if field in item_content:
                    texts.append(str(item_content[field]))

        # æ¨æ–‡ç‰¹å®šæ–‡æœ¬æå–
        item_content = content.get('itemContent', {})
        tweet_results = item_content.get('tweet_results', {})
        result = tweet_results.get('result', {})
        legacy = result.get('legacy', {})

        # 4. ä¸»æ¨æ–‡æ–‡æœ¬
        if 'full_text' in legacy:
            texts.append(legacy['full_text'])

        # 5. è½¬å‘æ¨æ–‡æ–‡æœ¬
        retweeted = legacy.get('retweeted_status_result', {}).get('result', {})
        if retweeted and 'legacy' in retweeted:
            texts.append(retweeted['legacy'].get('full_text', ''))

        # 6. å¼•ç”¨æ¨æ–‡æ–‡æœ¬
        quoted = legacy.get('quoted_status_result', {}).get('result', {})
        if quoted and 'legacy' in quoted:
            texts.append(quoted['legacy'].get('full_text', ''))

        # 7. ç¬”è®°æ¨æ–‡æ–‡æœ¬ï¼ˆé•¿æ–‡æœ¬ï¼‰
        note_tweet = result.get('note_tweet', {}).get('note_tweet_results', {}).get('result', {})
        if note_tweet and 'text' in note_tweet:
            texts.append(note_tweet['text'])

    except (KeyError, TypeError) as e:
        # å¿½ç•¥è·¯å¾„é”™è¯¯
        print(f"æå–æ–‡æœ¬æ—¶è·¯å¾„é”™è¯¯: {e}")

    return [t for t in texts if t]  # è¿‡æ»¤æ‰ç©ºæ–‡æœ¬


def filter_CommunitiesExploreTimeline(json_str, filter_words):
    """
    è¿‡æ»¤ CommunitiesExploreTimeline çš„JSONå“åº”
    """
    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    original_entry_count = 0
    filtered_entry_count = 0
    banned_matches = []

    try:
        print(f" å¼€å§‹è¿‡æ»¤ CommunitiesExploreTimeline æ•°æ®ï¼Œä½¿ç”¨ {len(filter_words)} ä¸ªè¿ç¦è¯")
        data = json.loads(json_str)

        # ç¡®ä¿æ•°æ®ç»“æ„å­˜åœ¨
        instructions = data.get('data', {}).get('viewer', {}).get('explore_communities_timeline', {}).get('timeline',
                                                                                                          {}).get(
            'instructions', [])

        # é¢„å¤„ç†è¿ç¦è¯ä¸ºå°å†™ï¼ˆä¸åŒºåˆ†å¤§å°å†™åŒ¹é…ï¼‰
        banned_pattern = re.compile(
            '|'.join(re.escape(word.lower()) for word in filter_words),
            re.IGNORECASE
        )

        # éå†æ‰€æœ‰æŒ‡ä»¤
        for instruction in instructions:
            if instruction.get('type') == 'TimelineAddEntries':
                entries = instruction.get('entries', [])
                original_entry_count = len(entries)

                if original_entry_count == 0:
                    print(" è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•æ¡ç›®æ•°æ®")
                    return json_str

                print(f" å‘ç° {original_entry_count} ä¸ªå¾…å¤„ç†æ¡ç›®")
                new_entries = []

                # éå†æ¯ä¸ªæ¡ç›®
                for idx, entry in enumerate(entries):
                    entry_id = entry.get('entryId', f"unknown_{idx}")
                    try:
                        # è·å–æ¨æ–‡å…¨æ–‡ï¼ˆå¯èƒ½ä½äºä¸åŒå±‚çº§ï¼‰
                        text_sources = []
                        content = entry.get('content', {})

                        # å±‚çº§1ï¼šç›´æ¥è·å–legacy.full_text
                        legacy = content.get('itemContent', {}).get('tweet_results', {}).get('result', {}).get('legacy',
                                                                                                               {})
                        if legacy:
                            text = legacy.get('full_text', "")
                            if text: text_sources.append(("æ¨æ–‡æ­£æ–‡", text))

                        # å±‚çº§2ï¼šå°è¯•è·å–ç”¨æˆ·æè¿°
                        user_desc = content.get('itemContent', {}).get('tweet_results', {}).get('result', {}).get(
                            'core', {}).get('user_results', {}).get('result', {}).get('legacy', {}).get('description',
                                                                                                        "")
                        if user_desc: text_sources.append(("ç”¨æˆ·æè¿°", user_desc))

                        # å±‚çº§3ï¼šå°è¯•è·å–ç¤¾åŒºæè¿°
                        community_desc = content.get('itemContent', {}).get('tweet_results', {}).get('result', {}).get(
                            'community_results', {}).get('result', {}).get('description', "")
                        if community_desc: text_sources.append(("ç¤¾åŒºæè¿°", community_desc))

                        # æ£€æŸ¥æ‰€æœ‰æ–‡æœ¬æ¥æº
                        found_banned = False
                        matched_words = set()

                        for source_type, text in text_sources:
                            # æ£€æŸ¥è¿ç¦è¯
                            matches = banned_pattern.findall(text.lower())
                            if matches:
                                found_banned = True
                                matched_words.update(matches)

                                # æ‰“å°åŒ¹é…è¯¦æƒ…
                                log_text = text[:50] + "..." if len(text) > 50 else text
                                print(f" æ£€æµ‹åˆ°è¿ç¦å†…å®¹ | æ¡ç›®ID: {entry_id}")
                                print(f"  æ¥æº: {source_type}")
                                print(f"  åŒ¹é…è¯æ±‡: {', '.join(matches)}")
                                print(f"  å†…å®¹ç‰‡æ®µ: '{log_text}'")

                        if found_banned:
                            filtered_entry_count += 1
                            banned_matches.append({
                                "entry_id": entry_id,
                                "matched_words": list(matched_words),
                                "sample_text": text_sources[0][1][:100] if text_sources else "æ— æ–‡æœ¬"
                            })
                            continue  # è·³è¿‡è¿ç¦æ¡ç›®

                        new_entries.append(entry)

                    except Exception as e:
                        print(f" å¤„ç†æ¡ç›® {entry_id} æ—¶å‡ºé”™: {str(e)}")
                        new_entries.append(entry)  # å‡ºé”™æ—¶ä¿ç•™åŸæ¡ç›®

                # æ›´æ–°è¿‡æ»¤åçš„æ¡ç›®
                instruction['entries'] = new_entries

        # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
        if filtered_entry_count == 0:
            print(f" è¿‡æ»¤å®Œæˆ: æ£€æŸ¥äº† {original_entry_count} ä¸ªæ¡ç›®ï¼Œæœªå‘ç°è¿ç¦å†…å®¹")
        else:
            print(f" è¿‡æ»¤å®Œæˆ: ç§»é™¤äº† {filtered_entry_count}/{original_entry_count} ä¸ªè¿ç¦æ¡ç›®")
            print(" è¿ç¦æ¡ç›®è¯¦æƒ…:")
            for match in banned_matches:
                print(f"  - æ¡ç›®ID: {match['entry_id']}")
                print(f"    åŒ¹é…è¯æ±‡: {', '.join(match['matched_words'])}")
                print(f"    å†…å®¹ç¤ºä¾‹: '{match['sample_text']}'")

        return json.dumps(data, ensure_ascii=False)

    except Exception as e:
        print(f"âŒ è¿‡æ»¤æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        return json_str


def filter_CommunitiesFetchOneQuery(response_body, filter_words):
    """
    è¿‡æ»¤CommunitiesFetchOneQueryç±»å‹çš„ç¤¾åŒºæ•°æ®ï¼Œä½¿ç”¨printè¾“å‡ºè¯¦ç»†æ“ä½œä¿¡æ¯
    """
    # ç”Ÿæˆå”¯ä¸€æ“ä½œID
    operation_id = f"OP-{int(time.time() * 1000)}"

    try:
        print(f"[{operation_id}]  å¼€å§‹å¤„ç†ç¤¾åŒºæ•°æ® | è¿ç¦è¯æ•°é‡: {len(filter_words)}")
        data = json.loads(response_body)
        community = data.get("data", {}).get("communityResults", {}).get("result")

        if not community:
            print(f"[{operation_id}] âš  æœªæ‰¾åˆ°ç¤¾åŒºæ•°æ®ï¼Œè·³è¿‡å¤„ç†")
            return response_body

        # æå–åŸºç¡€ä¿¡æ¯
        comm_id = community.get('id_str', 'æœªçŸ¥ID')
        comm_name = community.get('name', 'æœªå‘½åç¤¾åŒº')
        print(f"[{operation_id}]  æ­£åœ¨æ£€æŸ¥ç¤¾åŒº: ID={comm_id} | åç§°='{comm_name}'")

        # å‡†å¤‡æ£€æŸ¥çš„æ–‡æœ¬å­—æ®µ
        text_fields = []
        field_sources = []

        # æ·»åŠ åç§°å’Œæè¿°å­—æ®µ
        if name := community.get("name"):
            text_fields.append(name)
            field_sources.append(f"ç¤¾åŒºåç§°: '{name}'")

        if desc := community.get("description"):
            text_fields.append(desc)
            field_sources.append(f"ç¤¾åŒºæè¿°: '{desc[:50]}{'...' if len(desc) > 50 else ''}'")

        # æ·»åŠ è§„åˆ™å­—æ®µ
        for i, rule in enumerate(community.get("rules", [])):
            if rule_name := rule.get("name"):
                text_fields.append(rule_name)
                field_sources.append(f"è§„åˆ™#{i + 1}åç§°: '{rule_name}'")

            if rule_desc := rule.get("description"):
                text_fields.append(rule_desc)
                field_sources.append(f"è§„åˆ™#{i + 1}æè¿°: '{rule_desc[:50]}{'...' if len(rule_desc) > 50 else ''}'")

        print(f"[{operation_id}]  æ£€æŸ¥å­—æ®µæ•°: {len(text_fields)}")
        for i, source in enumerate(field_sources):
            print(f"    - [{i + 1}] {source}")

        # æ„å»ºè¿ç¦è¯æ­£åˆ™
        pattern = re.compile("|".join(filter_words), re.IGNORECASE)
        violations = []

        # æ£€æŸ¥æ¯ä¸ªæ–‡æœ¬å­—æ®µ
        for i, text in enumerate(text_fields):
            if match := pattern.search(text):
                field_name = field_sources[i].split(':')[0]
                matched_word = match.group(0)
                context_start = max(0, match.start() - 10)
                context_end = min(len(text), match.end() + 10)
                context = text[context_start:context_end].replace('\n', ' ')

                violations.append({
                    "field": field_name,
                    "word": matched_word,
                    "context": context,
                    "index": i + 1
                })

        # å¤„ç†è¿è§„æƒ…å†µ
        if violations:
            print(f"[{operation_id}]  å‘ç° {len(violations)} å¤„è¿è§„å†…å®¹!")
            for v in violations:
                print(f"        è¿è§„ #{v['index']}:")
                print(f"        å­—æ®µ: {v['field']}")
                print(f"        è¿ç¦è¯: '{v['word']}'")
                print(f"        ä¸Šä¸‹æ–‡: ...{v['context']}...")

            print(f"[{operation_id}]  åˆ é™¤ç¤¾åŒº: ID={comm_id} | åç§°='{comm_name}'")

            # æ¸…ç©ºç¤¾åŒºæ•°æ®ä½†ä¿ç•™ç»“æ„
            data["data"]["communityResults"]["result"] = None
            return json.dumps(data, ensure_ascii=False)

        # æ— è¿è§„æƒ…å†µ
        print(f"[{operation_id}]  ç¤¾åŒºæ£€æŸ¥é€šè¿‡ | ID={comm_id} | åç§°='{comm_name}'")
        print(f"    æ£€æŸ¥å­—æ®µ: {len(text_fields)} ä¸ª | æ— è¿ç¦è¯")
        return response_body

    except json.JSONDecodeError as e:
        print(f"[{operation_id}]  JSONè§£æå¤±è´¥: {str(e)}")
        return response_body
    except Exception as e:
        print(f"[{operation_id}]  å¤„ç†å¼‚å¸¸: {str(e)}")
        return response_body


def filter_CommunitiesRankedTimeline(json_str, filter_words):
    total_communities = 0
    filtered_count = 0
    filtered_details = []

    try:
        # å°†JSONå­—ç¬¦ä¸²è§£æä¸ºPythonå­—å…¸
        data = json.loads(json_str)

        # è·å–ç¤¾åŒºæ¡ç›®è·¯å¾„
        instructions = data["data"]["viewer"]["ranked_communities_timeline"]["timeline"]["instructions"]

        # éå†æ‰€æœ‰æŒ‡ä»¤
        for instruction in instructions:
            if instruction.get("type") == "TimelineAddEntries":
                entries = instruction.get("entries", [])
                new_entries = []

                # éå†æ‰€æœ‰æ¡ç›®
                for entry in entries:
                    # åªå¤„ç†ç¤¾åŒºæ¨¡å—æ¡ç›®
                    if entry.get("entryId", "").startswith("community-to-join-"):
                        content = entry.get("content", {})
                        items = content.get("items", [])
                        new_items = []

                        # æ£€æŸ¥æ¯ä¸ªç¤¾åŒºæ¡ç›®
                        for item in items:
                            total_communities += 1
                            item_content = item.get("item", {}).get("itemContent", {})
                            community_results = item_content.get("community_results", {})
                            result = community_results.get("result", {})

                            community_id = result.get("id_str", "unknown")
                            community_name = result.get("name", "unknown")

                            # æ£€æŸ¥å…³é”®å­—æ®µæ˜¯å¦åŒ…å«è¿ç¦è¯
                            should_keep = True
                            trigger_word = None
                            trigger_field = None
                            trigger_text = None

                            # æ£€æŸ¥åç§°ã€æè¿°å’Œé—®é¢˜å­—æ®µ
                            for field in ["name", "description", "question"]:
                                text = result.get(field, "")
                                for word in filter_words:
                                    if re.search(rf'\b{re.escape(word)}\b', text, re.IGNORECASE):
                                        should_keep = False
                                        trigger_word = word
                                        trigger_field = field
                                        trigger_text = text
                                        break
                                if not should_keep:
                                    break

                            # æ£€æŸ¥è§„åˆ™å­—æ®µ
                            if should_keep:
                                for rule in result.get("rules", []):
                                    rule_text = rule.get("name", "") + " " + rule.get("description", "")
                                    for word in filter_words:
                                        if re.search(rf'\b{re.escape(word)}\b', rule_text, re.IGNORECASE):
                                            should_keep = False
                                            trigger_word = word
                                            trigger_field = "rule"
                                            trigger_text = rule_text
                                            break
                                    if not should_keep:
                                        break

                            # å¤„ç†è¿‡æ»¤ç»“æœ
                            if should_keep:
                                new_items.append(item)
                            else:
                                filtered_count += 1
                                filtered_details.append({
                                    "id": community_id,
                                    "name": community_name,
                                    "trigger_word": trigger_word,
                                    "trigger_field": trigger_field,
                                    "trigger_text": trigger_text
                                })

                        # æ›´æ–°itemsåˆ—è¡¨
                        if new_items:
                            content["items"] = new_items
                            entry["content"] = content
                            new_entries.append(entry)
                        else:
                            # å¦‚æœæ¨¡å—ä¸­æ‰€æœ‰ç¤¾åŒºéƒ½è¢«è¿‡æ»¤ï¼Œè®°å½•æ•´ä¸ªæ¨¡å—è¢«ç§»é™¤
                            print(f"[ç¤¾åŒºè¿‡æ»¤] æ•´ä¸ªç¤¾åŒºæ¨¡å—è¢«ç§»é™¤ï¼ŒåŸåŒ…å« {len(items)} ä¸ªç¤¾åŒº")
                    else:
                        # ä¿ç•™éç¤¾åŒºæ¡ç›®
                        new_entries.append(entry)

                # æ›´æ–°entriesåˆ—è¡¨
                instruction["entries"] = new_entries

        # æ‰“å°è¿‡æ»¤ç»“æœæ‘˜è¦
        print("\n===== ç¤¾åŒºè¿‡æ»¤ç»“æœ =====")
        print(f"å…±æ£€æŸ¥ {total_communities} ä¸ªç¤¾åŒº")

        if filtered_count > 0:
            print(f" è¿‡æ»¤äº† {filtered_count} ä¸ªåŒ…å«è¿ç¦è¯çš„ç¤¾åŒº:")
            for i, detail in enumerate(filtered_details, 1):
                print(f"\nã€è¿è§„ç¤¾åŒº #{i}ã€‘")
                print(f"  ç¤¾åŒºID: {detail['id']}")
                print(f"  ç¤¾åŒºåç§°: '{detail['name']}'")
                print(f"  è§¦å‘è¿ç¦è¯: '{detail['trigger_word']}'")
                print(f"  è§¦å‘å­—æ®µ: {detail['trigger_field']}")
                print(f"  è¿è§„å†…å®¹: '{detail['trigger_text']}'")
        else:
            print("ğŸŸ¢ æœªå‘ç°åŒ…å«è¿ç¦è¯çš„ç¤¾åŒº")

        print("=======================\n")

        # è¿”å›è¿‡æ»¤åçš„JSONå­—ç¬¦ä¸²
        return json.dumps(data, ensure_ascii=False)

    except Exception as e:
        print(f" è¿‡æ»¤JSONæ—¶å‡ºé”™: {e}")
        # å‡ºé”™æ—¶è¿”å›åŸå§‹JSON
        return json_str


def filter_community_entry(entry, filter_words):
    """è¿‡æ»¤å•ä¸ªç¤¾åŒºæ¡ç›®ï¼Œè¿”å›è¿‡æ»¤åçš„æ¡ç›®å’Œè¿è§„ä¿¡æ¯"""
    try:
        # è·å–ç¤¾åŒºå†…å®¹çš„æ ¸å¿ƒå­—æ®µ
        content = entry.get("content", {})
        item_content = content.get("itemContent", {})
        community_results = item_content.get("community_results", {})
        result = community_results.get("result", {})

        community_id = result.get("id_str", "unknown")
        community_name = result.get("name", "unnamed community")

        # æ”¶é›†æ‰€æœ‰è¦æ£€æŸ¥çš„æ–‡æœ¬å­—æ®µ
        fields_to_check = {
            "åç§°": result.get("name", ""),
            "æè¿°": result.get("description", ""),
            "é—®é¢˜": result.get("question", "")
        }

        # æ·»åŠ æœç´¢æ ‡ç­¾
        tags = result.get("search_tags", [])
        if tags:
            fields_to_check["æœç´¢æ ‡ç­¾"] = ", ".join(tags)

        # æ·»åŠ ç¤¾åŒºè§„åˆ™
        rules = result.get("rules", [])
        for i, rule in enumerate(rules):
            fields_to_check[f"è§„åˆ™{i + 1}åç§°"] = rule.get("name", "")
            fields_to_check[f"è§„åˆ™{i + 1}æè¿°"] = rule.get("description", "")

        # æ£€æŸ¥æ‰€æœ‰å­—æ®µæ˜¯å¦åŒ…å«è¿ç¦è¯
        violations = []
        for field_name, field_value in fields_to_check.items():
            found_words = contains_filter_words(field_value, filter_words)
            if found_words:
                violations.append({
                    "field": field_name,
                    "value": field_value,
                    "words": found_words
                })

        # å¦‚æœæœ‰è¿è§„å†…å®¹
        if violations:
            # æ‰“å°è¿è§„è¯¦æƒ…
            print(f" å‘ç°è¿è§„ç¤¾åŒº: [ID: {community_id}, åç§°: '{community_name}']")
            for violation in violations:
                print(f"   â€¢ å­—æ®µ '{violation['field']}': å€¼ '{violation['value']}'")
                print(f"     è§¦å‘è¿ç¦è¯: {', '.join(violation['words'])}")
            return None  # åˆ é™¤è¯¥æ¡ç›®

        # æ²¡æœ‰è¿è§„å†…å®¹
        return entry

    except Exception as e:
        print(f" å¤„ç†ç¤¾åŒºæ¡ç›®æ—¶å‡ºé”™: {e}")
        return entry


def filter_CommunityDiscoveryTimeline(json_str, filter_words):
    """è¿‡æ»¤æ•´ä¸ªCommunityDiscoveryTimelineå“åº”"""
    try:
        data = json.loads(json_str)
        instructions = data.get("data", {}).get("viewer", {}).get("community_discovery_timeline", {}).get("timeline",
                                                                                                          {}).get(
            "instructions", [])

        total_entries = 0
        community_entries = 0
        filtered_count = 0

        print("=" * 50)
        print("å¼€å§‹è¿‡æ»¤ç¤¾åŒºå‘ç°æ—¶é—´çº¿æ•°æ®...")

        # æ‰¾åˆ°åŒ…å«ç¤¾åŒºæ¡ç›®çš„TimelineAddEntriesæŒ‡ä»¤
        for instruction in instructions:
            if instruction.get("type") == "TimelineAddEntries":
                entries = instruction.get("entries", [])
                total_entries = len(entries)
                filtered_entries = []

                print(f"å‘ç° {total_entries} ä¸ªæ¡ç›®")

                # è¿‡æ»¤æ¯ä¸ªç¤¾åŒºæ¡ç›®
                for entry in entries:
                    entry_id = entry.get("entryId", "unknown-entry")

                    # åªå¤„ç†ç¤¾åŒºç±»å‹çš„æ¡ç›®
                    if entry_id.startswith("community-"):
                        community_entries += 1
                        filtered_entry = filter_community_entry(entry, filter_words)

                        if filtered_entry is None:
                            filtered_count += 1
                        else:
                            filtered_entries.append(filtered_entry)
                    else:
                        # ä¿ç•™éç¤¾åŒºæ¡ç›®
                        filtered_entries.append(entry)

                # æ›´æ–°æ¡ç›®åˆ—è¡¨
                instruction["entries"] = filtered_entries

        # æ‰“å°è¿‡æ»¤ç»“æœ
        print("\nè¿‡æ»¤ç»“æœç»Ÿè®¡:")
        print(f"â€¢ æ€»æ¡ç›®æ•°: {total_entries}")
        print(f"â€¢ ç¤¾åŒºæ¡ç›®æ•°: {community_entries}")
        print(f"â€¢ è¿‡æ»¤ç¤¾åŒºæ•°: {filtered_count}")

        if filtered_count == 0:
            print("â€¢ çŠ¶æ€: æ²¡æœ‰å‘ç°éœ€è¦è¿‡æ»¤çš„å†…å®¹ ")
        else:
            print(f"â€¢ çŠ¶æ€: å·²è¿‡æ»¤ {filtered_count} ä¸ªç¤¾åŒº ")

        print("=" * 50)
        return json.dumps(data, ensure_ascii=False)

    except Exception as e:
        print(f" è§£æJSONæ—¶å‡ºé”™: {e}")
        return json_str


def filter_TopicTimelineQuery(json_str, filter_words):
    # è·å–å½“å‰æ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    total_communities = 0
    filtered_count = 0
    filtered_details = []

    try:
        # è§£æJSONæ•°æ®
        data = json.loads(json_str)

        # è·å–ç¤¾åŒºåˆ—è¡¨
        communities = data.get("data", {}).get("fetch_popular_communities", {}).get("items_results", [])
        total_communities = len(communities)

        # åˆ›å»ºè¿‡æ»¤åçš„ç¤¾åŒºåˆ—è¡¨
        filtered_communities = []

        # å°†è¿‡æ»¤è¯è½¬æ¢ä¸ºå°å†™ä»¥æé«˜åŒ¹é…æ•ˆç‡
        lower_filter_words = [word.lower() for word in filter_words]

        # æ‰“å°å¤„ç†å¼€å§‹ä¿¡æ¯
        print(f"[{timestamp}] å¼€å§‹è¿‡æ»¤ç¤¾åŒºæ•°æ® | åˆå§‹ç¤¾åŒºæ•°: {total_communities} | è¿‡æ»¤è¯: {filter_words}")

        # éå†æ‰€æœ‰ç¤¾åŒºæ¡ç›®
        for community in communities:
            # è·å–ç¤¾åŒºç»“æœå¯¹è±¡
            result = community.get("result", {})
            community_name = result.get("name", "")
            community_id = result.get("rest_id", "")

            # æ£€æŸ¥å…³é”®å­—æ®µæ˜¯å¦åŒ…å«è¿ç¦è¯
            violation_reason = None
            violation_word = None

            # æ£€æŸ¥ç¤¾åŒºåç§°
            name_lower = community_name.lower()
            for word in lower_filter_words:
                if word in name_lower:
                    violation_reason = "ç¤¾åŒºåç§°åŒ…å«è¿ç¦è¯"
                    violation_word = word
                    break

            # æ£€æŸ¥ä¸»è¯é¢˜åç§°
            if not violation_reason:
                topic = result.get("primary_community_topic", {})
                topic_name = topic.get("topic_name", "").lower()
                for word in lower_filter_words:
                    if word in topic_name:
                        violation_reason = "ä¸»é¢˜åç§°åŒ…å«è¿ç¦è¯"
                        violation_word = word
                        break

            # å¦‚æœä¸åŒ…å«è¿ç¦è¯ï¼Œåˆ™ä¿ç•™è¯¥ç¤¾åŒº
            if not violation_reason:
                filtered_communities.append(community)
            else:
                # è®°å½•è¿‡æ»¤è¯¦æƒ…
                filtered_count += 1
                detail = {
                    "id": community_id,
                    "name": community_name,
                    "reason": violation_reason,
                    "word": violation_word
                }
                filtered_details.append(detail)
                # æ‰“å°è¿‡æ»¤è­¦å‘Š
                print(f"[{timestamp}] !!! è¿‡æ»¤ç¤¾åŒº: ID={community_id} | åç§°='{community_name}' | "
                      f"åŸå› : {violation_reason} '{violation_word}'")

        # æ›´æ–°è¿‡æ»¤åçš„ç¤¾åŒºåˆ—è¡¨
        if "data" in data and "fetch_popular_communities" in data["data"]:
            data["data"]["fetch_popular_communities"]["items_results"] = filtered_communities

        # ç”Ÿæˆæœ€ç»ˆå¤„ç†ç»“æœ
        remaining_count = len(filtered_communities)

        if filtered_count > 0:
            # æ‰“å°è¿‡æ»¤æ‘˜è¦
            print(f"[{timestamp}] === è¿‡æ»¤å®Œæˆ ===")
            print(f"[{timestamp}] åŸå§‹ç¤¾åŒºæ•°: {total_communities} | è¿‡æ»¤: {filtered_count} | ä¿ç•™: {remaining_count}")

            # æ‰“å°è¿‡æ»¤è¯¦æƒ…
            print(f"[{timestamp}] è¿‡æ»¤è¯¦æƒ…:")
            for detail in filtered_details:
                print(f"    - ID: {detail['id']} | åç§°: '{detail['name']}' | "
                      f"åŸå› : {detail['reason']} | è¿ç¦è¯: '{detail['word']}'")
        else:
            # æ‰“å°æ— è¿‡æ»¤é€šçŸ¥
            print(f"[{timestamp}] === æœªå‘ç°éœ€è¿‡æ»¤å†…å®¹ ===")
            print(f"[{timestamp}] åŸå§‹ç¤¾åŒºæ•°: {total_communities} | è¿‡æ»¤: 0 | ä¿ç•™: {total_communities}")
            print(f"[{timestamp}] æ‰€æœ‰ç¤¾åŒºå‡ç¬¦åˆè¦æ±‚")

        # è¿”å›è¿‡æ»¤åçš„JSONå­—ç¬¦ä¸²
        return json.dumps(data, ensure_ascii=False)

    except Exception as e:
        # æ‰“å°é”™è¯¯ä¿¡æ¯
        print(f"[{timestamp}] !!! è¿‡æ»¤JSONæ—¶å‡ºé”™: {str(e)}")
        print(f"[{timestamp}] ç”±äºå¤„ç†å‡ºé”™ï¼Œè¿”å›åŸå§‹æœªè¿‡æ»¤æ•°æ®")
        # å‡ºé”™æ—¶è¿”å›åŸå§‹æ•°æ®
        return json_str


def filter_CommunitiesSearchQuery(json_str, filter_words):
    try:
        # è§£æJSONæ•°æ®
        data = json.loads(json_str)
        original_count = len(data.get("data", {}).get("communities_search_slice", {}).get("items_results", []))

        print(f" å¼€å§‹è¿‡æ»¤ç¤¾åŒºæœç´¢ç»“æœï¼ŒåŸå§‹æ¡ç›®æ•°: {original_count}")
        print(f" ä½¿ç”¨è¿ç¦è¯åˆ—è¡¨: {filter_words}")

        # è·å–ç¤¾åŒºåˆ—è¡¨
        communities = data.get("data", {}).get("communities_search_slice", {}).get("items_results", [])

        # è¿‡æ»¤åŒ…å«è¿ç¦è¯çš„ç¤¾åŒº
        filtered_communities = []
        removed_count = 0
        removed_names = []

        for community in communities:
            result = community.get("result", {})
            community_name = result.get("name", "")
            lower_name = community_name.lower()

            # æ£€æŸ¥ç¤¾åŒºåç§°æ˜¯å¦åŒ…å«è¿ç¦è¯
            found_bad_words = [word for word in filter_words if word in lower_name]

            if found_bad_words:
                removed_count += 1
                removed_names.append(community_name)
                print(f" åˆ é™¤ç¤¾åŒº: '{community_name}' | åŒ…å«è¿ç¦è¯: {found_bad_words}")
            else:
                filtered_communities.append(community)

        # é‡å»ºè¿‡æ»¤åçš„æ•°æ®ç»“æ„
        if "data" in data and "communities_search_slice" in data["data"]:
            data["data"]["communities_search_slice"]["items_results"] = filtered_communities

        # æ‰“å°è¿‡æ»¤ç»“æœæ‘˜è¦
        print(f" è¿‡æ»¤å®Œæˆ: åˆ é™¤ {removed_count} ä¸ªç¤¾åŒºï¼Œä¿ç•™ {len(filtered_communities)} ä¸ªç¤¾åŒº")
        if removed_count > 0:
            print(f"ğŸ—‘ è¢«åˆ é™¤çš„ç¤¾åŒºåˆ—è¡¨: {removed_names}")
        else:
            print(" æœªå‘ç°åŒ…å«è¿ç¦è¯çš„ç¤¾åŒº")

        return json.dumps(data, ensure_ascii=False)

    except json.JSONDecodeError:
        print("ï¸ JSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®")
        return json_str
    except Exception as e:
        print(f" è¿‡æ»¤ç¤¾åŒºæ•°æ®æ—¶å‡ºé”™: {e}")
        return json_str


def filter_community_tweets(json_str, forbidden_words):
    """
    è¿‡æ»¤ç¤¾åŒºæ¨æ–‡æ—¶é—´çº¿ä¸­åŒ…å«è¿ç¦è¯çš„æ¡ç›®
    """
    # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_entries': 0,
        'pinned_entries': 0,
        'removed_entries': 0,
        'removed_contents': []
    }

    def process_entry(entry):
        """å¤„ç†å•ä¸ªæ¡ç›®ï¼Œè¿”å›æ˜¯å¦ä¿ç•™"""
        try:
            # åªå¤„ç†æ¨æ–‡ç±»å‹çš„æ¡ç›®
            entry_id = entry.get("entryId", "")
            stats['total_entries'] += 1

            if not entry_id.startswith("tweet-"):
                return True  # éæ¨æ–‡æ¡ç›®ä¿ç•™

            content = entry["content"]
            # éªŒè¯æ¡ç›®ç»“æ„
            if (content.get("entryType") == "TimelineTimelineItem" and
                    content.get("itemContent", {}).get("itemType") == "TimelineTweet"):

                tweet = content["itemContent"]["tweet_results"]["result"]
                tweet_text = tweet["legacy"]["full_text"]

                # æ£€æŸ¥æ˜¯å¦æ˜¯ç½®é¡¶æ¡ç›®
                is_pinned = "socialContext" in content.get("itemContent", {})
                if is_pinned:
                    stats['pinned_entries'] += 1


                if contains_forbidden_text(tweet_text, forbidden_words):
                    # è®°å½•è¢«åˆ é™¤çš„å†…å®¹
                    truncated_text = tweet_text[:100] + ('...' if len(tweet_text) > 100 else '')
                    author = tweet["core"]["user_results"]["result"]["legacy"]["screen_name"]
                    removed_info = {
                        'entry_id': entry_id,
                        'author': author,
                        'text': truncated_text,
                        'is_pinned': is_pinned
                    }
                    stats['removed_contents'].append(removed_info)
                    stats['removed_entries'] += 1
                    return False
        except KeyError:
            # ç»“æ„å¼‚å¸¸æ—¶ä¿ç•™æ¡ç›®
            pass
        return True

    try:
        # æ‰“å°å¼€å§‹å¤„ç†ä¿¡æ¯
        print(f" å¼€å§‹è¿‡æ»¤ç¤¾åŒºæ¨æ–‡ï¼Œè¿ç¦è¯åˆ—è¡¨: {forbidden_words}")

        data = json.loads(json_str)
        timeline = data["data"]["communityResults"]["result"]["ranked_community_timeline"]["timeline"]
        new_instructions = []

        for instruction in timeline["instructions"]:
            # å¤„ç†ç½®é¡¶æ¨æ–‡
            if instruction["type"] == "TimelinePinEntry":
                if process_entry(instruction["entry"]):
                    new_instructions.append(instruction)

            # å¤„ç†å¸¸è§„æ¨æ–‡åˆ—è¡¨
            elif instruction["type"] == "TimelineAddEntries":
                original_count = len(instruction["entries"])
                instruction["entries"] = [
                    entry for entry in instruction["entries"]
                    if process_entry(entry)
                ]
                new_instructions.append(instruction)

            # ä¿ç•™å…¶ä»–ç±»å‹æŒ‡ä»¤
            else:
                new_instructions.append(instruction)

        timeline["instructions"] = new_instructions
        result_json = json.dumps(data, ensure_ascii=False)

        # æ‰“å°å¤„ç†ç»“æœç»Ÿè®¡
        print(f"  è¿‡æ»¤å®Œæˆ! å…±å¤„ç† {stats['total_entries']} ä¸ªæ¡ç›®")
        print(f"   - ç½®é¡¶æ¡ç›®: {stats['pinned_entries']}")
        print(f"   - åˆ é™¤æ¡ç›®: {stats['removed_entries']}")

        # æ‰“å°è¢«åˆ é™¤çš„æ¡ç›®è¯¦æƒ…
        if stats['removed_entries'] > 0:
            print("\n è¢«åˆ é™¤çš„æ¡ç›®:")
            for i, content in enumerate(stats['removed_contents'], 1):
                pinned_tag = " [ç½®é¡¶]" if content['is_pinned'] else ""
                print(f"  {i}. ä½œè€…: @{content['author']}{pinned_tag}")
                print(f"     æ¡ç›®ID: {content['entry_id']}")
                print(f"     å†…å®¹: '{content['text']}'")
                print("-" * 50)
        else:
            print("ğŸ‰ æœªå‘ç°åŒ…å«è¿ç¦è¯çš„æ¡ç›®")

        return result_json

    except Exception as e:
        print(f" è¿‡æ»¤ç¤¾åŒºæ¨æ–‡æ—¶å‡ºé”™: {e}")
        return json_str  # å‡ºé”™æ—¶è¿”å›åŸå§‹æ•°æ®


def filter_ListsManagementPageTimeline(json_str, filter_words):
    if not filter_words:
        print("æ— è¿‡æ»¤è¯ï¼Œè·³è¿‡åˆ—è¡¨è¿‡æ»¤")
        return json_str

    print(f"å¼€å§‹è¿‡æ»¤åˆ—è¡¨ç®¡ç†é¡µé¢æ—¶é—´çº¿ï¼Œè¿‡æ»¤è¯: {filter_words}")
    start_time = time.time()
    original_size = len(json_str)
    removed_count = 0
    kept_count = 0

    try:
        data = json.loads(json_str)
        instructions = data["data"]["viewer"]["list_management_timeline"]["timeline"]["instructions"]

        for instruction in instructions:
            if instruction["type"] == "TimelineAddEntries":
                entries = instruction["entries"]
                new_entries = []

                for entry in entries:
                    entry_id = entry["entryId"]

                    # å¤„ç†æ¨èåˆ—è¡¨æ¨¡å—
                    if entry_id.startswith("list-to-follow-module-"):
                        print(f"å¤„ç†æ¨èåˆ—è¡¨æ¨¡å—: {entry_id}")
                        items = entry["content"]["items"]
                        original_item_count = len(items)
                        new_items = []

                        for item in items:
                            list_data = item["item"]["itemContent"].get("list")
                            if list_data:
                                list_name = list_data["name"]
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡æ»¤è¯
                                matched = False
                                for word in filter_words:
                                    if re.search(rf'\b{re.escape(word)}\b', list_name, re.IGNORECASE):
                                        print(f"  Ã— ç§»é™¤æ¨èåˆ—è¡¨: {list_name} (åŒ¹é…è¿‡æ»¤è¯: {word})")
                                        removed_count += 1
                                        matched = True
                                        break

                                if not matched:
                                    new_items.append(item)
                                    kept_count += 1

                        # æ›´æ–°æ¨¡å—å†…å®¹
                        entry["content"]["items"] = new_items
                        final_item_count = len(new_items)

                        if final_item_count > 0:
                            new_entries.append(entry)
                            print(f"  ä¿ç•™æ¨èåˆ—è¡¨: {final_item_count}/{original_item_count} ä¸ª")
                        else:
                            print(f"  ! æ¨èåˆ—è¡¨æ¨¡å—å·²ç©ºï¼Œå®Œå…¨ç§»é™¤")

                    # å¤„ç†ç”¨æˆ·åˆ—è¡¨æ¨¡å—
                    elif entry_id.startswith("owned-subscribed-list-module-"):
                        print(f"å¤„ç†ç”¨æˆ·åˆ—è¡¨æ¨¡å—: {entry_id}")
                        items = entry["content"]["items"]
                        original_item_count = len(items)
                        new_items = []

                        for item in items:
                            item_content = item["item"]["itemContent"]
                            # ä¿ç•™ç©ºåˆ—è¡¨æç¤ºä¿¡æ¯
                            if item_content.get("itemType") == "TimelineMessagePrompt":
                                print("  ä¿ç•™ç©ºåˆ—è¡¨æç¤ºä¿¡æ¯")
                                new_items.append(item)
                                kept_count += 1
                            # è¿‡æ»¤å®é™…åˆ—è¡¨é¡¹
                            elif item_content.get("list"):
                                list_name = item_content["list"]["name"]
                                matched = False
                                for word in filter_words:
                                    if re.search(rf'\b{re.escape(word)}\b', list_name, re.IGNORECASE):
                                        print(f"  Ã— ç§»é™¤ç”¨æˆ·åˆ—è¡¨: {list_name} (åŒ¹é…è¿‡æ»¤è¯: {word})")
                                        removed_count += 1
                                        matched = True
                                        break

                                if not matched:
                                    new_items.append(item)
                                    kept_count += 1

                        # æ›´æ–°æ¨¡å—å†…å®¹
                        entry["content"]["items"] = new_items
                        new_entries.append(entry)
                        final_item_count = len(new_items)

                        if original_item_count > 0:
                            print(f"  ç”¨æˆ·åˆ—è¡¨ä¿ç•™: {final_item_count}/{original_item_count} ä¸ª")


                # æ›´æ–°æŒ‡ä»¤æ¡ç›®
                instruction["entries"] = new_entries
                print(f"æ—¶é—´çº¿æ¡ç›®æ›´æ–°: {len(new_entries)}/{len(entries)} ä¸ªæ¡ç›®ä¿ç•™")

        filtered_json = json.dumps(data, ensure_ascii=False)
        filtered_size = len(filtered_json)

        # ç»“æœç»Ÿè®¡
        duration = time.time() - start_time
        print(f"è¿‡æ»¤å®Œæˆ! è€—æ—¶: {duration:.4f}ç§’")
        print(f"åŸå§‹å¤§å°: {original_size} å­—èŠ‚ | è¿‡æ»¤å: {filtered_size} å­—èŠ‚")
        print(f"å¤„ç†æ¡ç›®: ä¿ç•™ {kept_count} ä¸ª | ç§»é™¤ {removed_count} ä¸ª")

        if removed_count == 0:
            print("â˜… æœªç§»é™¤ä»»ä½•åˆ—è¡¨ï¼Œæ‰€æœ‰å†…å®¹å‡ç¬¦åˆè¿‡æ»¤è§„åˆ™")

        return filtered_json

    except Exception as e:
        print(f"!! è¿‡æ»¤è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return json_str



def filter_tweet_detail(response_body, filter_words):
    """è¿‡æ»¤æ¨æ–‡è¯¦æƒ…é¡µJSONï¼ˆunknown_TweetDetail.jsonï¼‰"""
    try:
        data = json.loads(response_body)

        # æ£€æŸ¥å“åº”ç»“æ„æ˜¯å¦æœ‰æ•ˆ
        if "data" not in data or "threaded_conversation_with_injections_v2" not in data["data"]:
            return response_body, False

        instructions = data["data"]["threaded_conversation_with_injections_v2"]["instructions"]
        deleted_items = []
        main_tweet_id = None

        # å°†è¿‡æ»¤è¯è½¬ä¸ºå°å†™
        filter_words = [word.lower() for word in filter_words]

        # 1. å¢å¼ºçš„ä¸»æ¨æ–‡IDæŸ¥æ‰¾é€»è¾‘ï¼ˆä¸‰å±‚æŸ¥æ‰¾ï¼‰
        for instruction in instructions:
            if instruction["type"] == "TimelineAddEntries":
                for entry in instruction["entries"]:
                    entry_id = entry["entryId"]

                    # æƒ…å†µ1: ç›´æ¥ä»¥tweet-å¼€å¤´çš„æ¡ç›®
                    if entry_id.startswith("tweet-"):
                        parts = entry_id.split("-")
                        if len(parts) > 1:
                            main_tweet_id = parts[1]
                            break

                    # æƒ…å†µ2: conversationthread-æ¡ç›®ä¸­çš„ç¬¬ä¸€ä¸ªitem
                    elif entry_id.startswith("conversationthread-"):
                        items = entry["content"].get("items", [])
                        if items:
                            first_item_id = items[0].get("entryId", "")
                            if "-tweet-" in first_item_id:
                                main_tweet_id = first_item_id.split("-tweet-")[-1]
                                break
                if main_tweet_id:
                    break

        # æƒ…å†µ3: å¦‚æœä»æœªæ‰¾åˆ°ï¼Œå°è¯•æ·±åº¦æœç´¢æ¨æ–‡æ•°æ®
        if not main_tweet_id:
            for instruction in instructions:
                if instruction["type"] == "TimelineAddEntries":
                    for entry in instruction["entries"]:
                        tweet_data = extract_tweet_data(entry)
                        if tweet_data and tweet_data.get("rest_id"):
                            main_tweet_id = tweet_data["rest_id"]
                            break
                    if main_tweet_id:
                        break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»æ¨æ–‡IDï¼Œè¿”å›åŸå§‹å“åº”
        if not main_tweet_id:
            print("Warning: Main tweet ID not found")
            return response_body, False

        # 2. ä¼˜å…ˆæ£€æŸ¥ä¸»æ¨æ–‡æ˜¯å¦å­˜åœ¨è¿ç¦è¯
        main_tweet_deleted = False
        for instruction in instructions:
            if instruction["type"] == "TimelineAddEntries":
                for entry in instruction["entries"]:
                    tweet_data = extract_tweet_data(entry)
                    if tweet_data and tweet_data.get("rest_id") == main_tweet_id:
                        # æå–æ¨æ–‡å†…å®¹
                        tweet_text = get_tweet_text(tweet_data).lower()
                        user_info = get_user_info(tweet_data).lower()

                        # æ‰“å°è°ƒè¯•ä¿¡æ¯
                        print(f"Checking main tweet: ID={main_tweet_id}")
                        print(f"  Tweet text: {tweet_text[:100]}...")
                        print(f"  User info: {user_info[:100]}...")

                        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡æ»¤è¯
                        for word in filter_words:
                            if word in tweet_text or word in user_info:
                                print(f"  Banned word found: '{word}'")
                                main_tweet_deleted = True
                                break

                        if main_tweet_deleted:
                            deleted_items.append({
                                "type": "Main Tweet",
                                "id": entry["entryId"],
                                "text": tweet_text[:50] + "...",
                                "user": get_user_screen_name(tweet_data)
                            })
                            break
                if main_tweet_deleted:
                    break

        # 3. å¦‚æœä¸»æ¨æ–‡åŒ…å«è¿ç¦è¯ï¼Œè¿”å›"æ¨æ–‡ä¸å­˜åœ¨"çš„å“åº”
        if main_tweet_deleted:
            print(f"Main tweet contains banned words, returning 'Tweet unavailable' response")
            # è¿”å›"æ¨æ–‡ä¸å¯ç”¨"å“åº”å¹¶å¼ºåˆ¶ç¦ç”¨ç¼“å­˜
            return create_tweet_unavailable_response(main_tweet_id), True

        # 4. ä¸»æ¨æ–‡å®‰å…¨ï¼Œç»§ç»­è¿‡æ»¤å…¶ä»–åŒºåŸŸ
        print("Main tweet is safe, filtering replies and recommendations...")
        for instruction in instructions:
            if instruction["type"] == "TimelineAddEntries":
                entries = instruction["entries"]
                new_entries = []

                for entry in entries:
                    entry_id = entry["entryId"]
                    tweet_data = extract_tweet_data(entry)

                    # ä¿ç•™ä¸»æ¨æ–‡ï¼ˆå·²æ£€æŸ¥è¿‡å®‰å…¨ï¼‰
                    if tweet_data and tweet_data.get("rest_id") == main_tweet_id:
                        new_entries.append(entry)
                        continue

                    # è¿‡æ»¤å›å¤åŒºåŸŸï¼ˆè¯„è®ºï¼‰
                    if entry_id.startswith("conversationthread-"):
                        items = entry["content"].get("items", [])
                        new_items = []

                        for item in items:
                            item_content = item.get("item", {}).get("itemContent", {})
                            tweet_data = item_content.get("tweet_results", {}).get("result", {})

                            # è·³è¿‡ä¸»æ¨æ–‡ï¼ˆå·²å¤„ç†ï¼‰
                            if tweet_data and tweet_data.get("rest_id") == main_tweet_id:
                                new_items.append(item)
                                continue

                            if should_delete_tweet(tweet_data, filter_words):
                                deleted_items.append({
                                    "type": "Reply",
                                    "id": item.get("entryId", ""),
                                    "text": get_tweet_text(tweet_data)[:50] + "...",
                                    "user": get_user_screen_name(tweet_data)
                                })
                            else:
                                new_items.append(item)

                        # å¦‚æœå¯¹è¯ä¸­æ‰€æœ‰å›å¤éƒ½è¢«åˆ é™¤ï¼Œä½†åŒ…å«ä¸»æ¨æ–‡åˆ™ä¿ç•™
                        if not new_items:
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸»æ¨æ–‡
                            has_main_tweet = any(
                                item.get("item", {}).get("itemContent", {}).get("tweet_results", {}).get("result",
                                                                                                         {}).get(
                                    "rest_id") == main_tweet_id
                                for item in items
                            )
                            if has_main_tweet:
                                new_items = [item for item in items if
                                             item.get("item", {}).get("itemContent", {}).get("tweet_results", {}).get(
                                                 "result", {}).get("rest_id") == main_tweet_id]

                        # æ›´æ–°æ¡ç›®ä¸­çš„items
                        if new_items:
                            entry["content"]["items"] = new_items
                            new_entries.append(entry)

                    # è¿‡æ»¤Relevant PeopleåŒºåŸŸ
                    elif entry_id.startswith("user-recommendations-"):
                        items = entry["content"].get("items", [])
                        new_items = []

                        for item in items:
                            user_data = item.get("item", {}).get("itemContent", {}).get("user_results", {}).get(
                                "result", {})
                            user_info = get_user_info(user_data).lower()

                            if any(word in user_info for word in filter_words):
                                deleted_items.append({
                                    "type": "Relevant People",
                                    "id": item.get("entryId", ""),
                                    "user": get_user_screen_name(user_data),
                                    "description": user_info[:50] + "..."
                                })
                            else:
                                new_items.append(item)

                        # å¦‚æœæ‰€æœ‰æ¨èç”¨æˆ·éƒ½è¢«åˆ é™¤ï¼Œè·³è¿‡æ•´ä¸ªæ¡ç›®
                        if new_items:
                            entry["content"]["items"] = new_items
                            new_entries.append(entry)
                    else:
                        # ä¿ç•™å…¶ä»–ç±»å‹çš„æ¡ç›®
                        new_entries.append(entry)

                # æ›´æ–°æŒ‡ä»¤ä¸­çš„æ¡ç›®
                instruction["entries"] = new_entries

        # æ‰“å°åˆ é™¤çš„é¡¹ç›®ä¿¡æ¯
        if deleted_items:
            print(f"Filtered {len(deleted_items)} items from tweet_detail:")
            for item in deleted_items:
                if item["type"] == "Main Tweet":
                    print(f"  - MAIN TWEET by @{item['user']}: {item['text']}")
                elif item["type"] == "Reply":
                    print(f"  - REPLY by @{item['user']}: {item['text']}")
                elif item["type"] == "Relevant People":
                    print(f"  - USER RECOMMENDATION: @{item['user']} - {item['description']}")

        return json.dumps(data), False

    except Exception as e:
        print(f"Error filtering tweet_detail: {str(e)}")
        import traceback
        traceback.print_exc()
        return response_body, False


def extract_tweet_data(entry):
    """ä»ä¸åŒæ¡ç›®ç»“æ„ä¸­æå–æ¨æ–‡æ•°æ®"""
    content = entry.get("content", {})

    # ç±»å‹1: ç›´æ¥åŒ…å«itemContentçš„æ¡ç›®
    if "itemContent" in content:
        return content["itemContent"].get("tweet_results", {}).get("result", {})

    # ç±»å‹2: conversationthreadæ¡ç›®ä¸­çš„items
    if "items" in content:
        for item in content["items"]:
            item_content = item.get("item", {}).get("itemContent", {})
            if item_content:
                tweet_data = item_content.get("tweet_results", {}).get("result", {})
                if tweet_data:
                    return tweet_data

    # ç±»å‹3: TimelineTimelineItemç»“æ„
    if content.get("__typename") == "TimelineTimelineItem" and "itemContent" in content:
        return content["itemContent"].get("tweet_results", {}).get("result", {})

    return None


def create_tweet_unavailable_response(tweet_id):
    """åˆ›å»º'æ¨æ–‡ä¸å¯ç”¨'çš„æ ‡å‡†å“åº”"""
    return json.dumps({
        "data": {
            "threaded_conversation_with_injections_v2": {
                "instructions": [
                    {
                        "type": "TimelineAddEntries",
                        "entries": [
                            {
                                "entryId": f"tweet-{tweet_id}",
                                "content": {
                                    "entryType": "TimelineTimelineItem",
                                    "itemContent": {
                                        "itemType": "TimelineTombstone",
                                        "__typename": "TimelineTombstone",
                                        "tombstoneInfo": {
                                            "richText": {
                                                "text": "This Tweet is unavailable.",
                                                "entities": []
                                            }
                                        }
                                    }
                                }
                            }
                        ]
                    }
                ]
            }
        }
    })


def get_tweet_text(tweet_data):
    """ä»æ¨æ–‡æ•°æ®ä¸­æå–æ–‡æœ¬"""
    if not tweet_data:
        return ""

    # å°è¯•ä»legacyå­—æ®µè·å–
    if "legacy" in tweet_data:
        return tweet_data["legacy"].get("full_text", "")

    # å°è¯•ä»note_tweetå­—æ®µè·å–
    if "note_tweet" in tweet_data:
        return tweet_data["note_tweet"].get("text", "")

    # å°è¯•ä»æ ¸å¿ƒå­—æ®µè·å–
    return tweet_data.get("text", "")


def get_user_info(user_data):
    """ä»ç”¨æˆ·æ•°æ®ä¸­æå–ä¿¡æ¯ï¼ˆç”¨æˆ·å+æè¿°ï¼‰"""
    if not user_data:
        return ""

    screen_name = ""
    description = ""

    # ä»ä¸åŒä½ç½®æå–ç”¨æˆ·å
    if "core" in user_data and "screen_name" in user_data["core"]:
        screen_name = user_data["core"]["screen_name"]
    elif "legacy" in user_data and "screen_name" in user_data["legacy"]:
        screen_name = user_data["legacy"]["screen_name"]
    elif "screen_name" in user_data:
        screen_name = user_data["screen_name"]

    # æå–ç”¨æˆ·æè¿°
    if "legacy" in user_data and "description" in user_data["legacy"]:
        description = user_data["legacy"]["description"]
    elif "description" in user_data:
        description = user_data["description"]

    return f"{screen_name} {description}"


def get_user_screen_name(tweet_data):
    """è·å–æ¨æ–‡å‘å¸ƒè€…çš„ç”¨æˆ·å"""
    if not tweet_data:
        return "unknown"

    # ä»æ ¸å¿ƒç”¨æˆ·æ•°æ®è·å–
    if "core" in tweet_data and "user_results" in tweet_data["core"]:
        user_data = tweet_data["core"]["user_results"].get("result", {})
        if "core" in user_data and "screen_name" in user_data["core"]:
            return user_data["core"]["screen_name"]
        elif "legacy" in user_data and "screen_name" in user_data["legacy"]:
            return user_data["legacy"]["screen_name"]
        elif "screen_name" in user_data:
            return user_data["screen_name"]

    # å°è¯•ç›´æ¥ä»æ¨æ–‡æ•°æ®è·å–
    return tweet_data.get("screen_name", "unknown")


def should_delete_tweet(tweet_data, filter_words):
    """æ£€æŸ¥æ¨æ–‡æ˜¯å¦åº”è¢«åˆ é™¤"""
    if not tweet_data:
        return False

    # è·å–æ¨æ–‡æ–‡æœ¬
    tweet_text = get_tweet_text(tweet_data).lower()

    # è·å–ç”¨æˆ·ä¿¡æ¯
    user_info = get_user_info(tweet_data).lower()

    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡æ»¤è¯
    return any(word in tweet_text or word in user_info for word in filter_words)
